{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def normalize(p, q):\n",
    "    p, q = np.asarray(p), np.asarray(q)\n",
    "    assert (p >= 0).all(), p\n",
    "    assert (q >= 0).all()\n",
    "    p, q = p / p.sum(), q / q.sum()\n",
    "    return p, q\n",
    "\n",
    "def JSD(p, q, base=2):\n",
    "    p, q = normalize(p, q)\n",
    "    m = 1. / 2 * (p + q)\n",
    "    return stats.entropy(p, m, base=base) / 2. + stats.entropy(q, m, base=base) / 2.\n",
    "\n",
    "def rnorm_sum_squares(x, y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    assert x.shape == y.shape\n",
    "    numerator = np.sum(np.square(x - y))\n",
    "    denominator = np.sum(np.square(x) + np.square(y))\n",
    "    rnorm_ss = np.sqrt(numerator / denominator)\n",
    "    return rnorm_ss\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    metric = []\n",
    "    ndcg=ndcg_score([y_true], [y_pred])\n",
    "    Rjsd=1-JSD(y_true, y_pred, base=2) \n",
    "    RRNSS = 1-rnorm_sum_squares(y_true, y_pred)\n",
    "    metric.append(ndcg)\n",
    "    metric.append(Rjsd)\n",
    "    metric.append(RRNSS)\n",
    "    return metric  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(y_pred):    \n",
    "    for i in range(len(y_pred)):    \n",
    "        if y_pred[i] < 0.5:\n",
    "            y_pred[i] = 0.0\n",
    "        elif y_pred[i] < 1.5:\n",
    "            y_pred[i] = 1.0\n",
    "        elif y_pred[i] < 2.5:\n",
    "            y_pred[i] = 2.0\n",
    "        elif y_pred[i] < 3.5:\n",
    "            y_pred[i] = 3.0\n",
    "        else:\n",
    "            y_pred[i] = 4.0            \n",
    "    return y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "class GloVeFeatXGBoost():\n",
    "    def __init__(self, embedding_size=300, n_estimators=100, max_depth=3, learning_rate=0.1):\n",
    "        self.glove_file = 'G:/L2\\glove\\glove.6B.300d.txt'\n",
    "        self.glove_model = self.load_glove_model(self.glove_file)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.clf = XGBRegressor(\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_depth=self.max_depth,\n",
    "            learning_rate=self.learning_rate\n",
    "        )\n",
    "\n",
    "    def load_glove_model(self, glove_file):\n",
    "        model = {}\n",
    "        with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                split_line = line.strip().split()\n",
    "                word = split_line[0]\n",
    "                embedding = np.array([float(val) for val in split_line[1:]])\n",
    "                model[word] = embedding\n",
    "        return model\n",
    "\n",
    "    def _get_sentence_embedding(self, sentence):\n",
    "        vecs = [self.glove_model.get(word, np.zeros(self.embedding_size)) for word in sentence]\n",
    "        vecs = np.array(vecs)\n",
    "        return np.mean(vecs, axis=0)\n",
    "\n",
    "    def train(self, X_train_text, X_train_feat, y_train):\n",
    "        X_train_vecs = np.array([self._get_sentence_embedding(sentence) for sentence in X_train_text])\n",
    "        X_train_vecs = np.hstack((X_train_vecs, X_train_feat))\n",
    "        self.clf.fit(X_train_vecs, y_train)\n",
    "\n",
    "    def predict(self, X_test_text, X_test_feat):\n",
    "        X_test_vecs = np.array([self._get_sentence_embedding(sentence) for sentence in X_test_text])\n",
    "        X_test_vecs = np.hstack((X_test_vecs, X_test_feat))\n",
    "        return discretize(self.clf.predict(X_test_vecs))\n",
    "\n",
    "    def evaluate(self, X_test_text, X_test_feat, y_test):\n",
    "        y_pred = self.predict(X_test_text, X_test_feat)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "        recall = recall_score(y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')        \n",
    "        return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "class GloVeXGBoost():\n",
    "    def __init__(self, embedding_size=300, n_estimators=100, max_depth=3, learning_rate=0.1):\n",
    "        self.glove_file = 'G:/L2\\glove\\glove.6B.300d.txt'\n",
    "        self.glove_model = self.load_glove_model(self.glove_file)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.clf = XGBRegressor(\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_depth=self.max_depth,\n",
    "            learning_rate=self.learning_rate\n",
    "        )\n",
    "\n",
    "    def load_glove_model(self, glove_file):\n",
    "        model = {}\n",
    "        with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                split_line = line.strip().split()\n",
    "                word = split_line[0]\n",
    "                embedding = np.array([float(val) for val in split_line[1:]])\n",
    "                model[word] = embedding\n",
    "        return model\n",
    "\n",
    "    def _get_sentence_embedding(self, sentence):\n",
    "        sentence_str = ' '.join(sentence)\n",
    "        vecs = [self.glove_model[word] for word in sentence_str.split() if word in self.glove_model]\n",
    "        vecs = np.array(vecs)\n",
    "        return np.mean(vecs, axis=0)\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        X_train_vecs = np.array([self._get_sentence_embedding(sentence) for sentence in X_train])\n",
    "        self.clf.fit(X_train_vecs, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test_vecs = np.array([self._get_sentence_embedding(sentence) for sentence in X_test])\n",
    "        return discretize(self.clf.predict(X_test_vecs))\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "        recall = recall_score(y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')        \n",
    "        return accuracy, precision, recall, f1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "class GloVeFeatSVM():\n",
    "    def __init__(self,  kernel='linear', C=1):\n",
    "        # Load the GloVe word vector model\n",
    "        self.glove_file='G:/L2\\glove\\glove.6B.300d.txt'\n",
    "        self.glove_model = self.load_glove_model(self.glove_file)\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.clf = SVC(kernel=self.kernel, C=self.C)\n",
    "\n",
    "    def load_glove_model(self, glove_file):\n",
    "        with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            glove_model = {}\n",
    "            for line in lines:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coef = np.asarray(values[1:], dtype='float32')\n",
    "                glove_model[word] = coef\n",
    "        return glove_model\n",
    "    \n",
    "    def _get_sentence_embedding(self, sentence):\n",
    "        sentence_str = ' '.join(sentence)\n",
    "        vecs = [self.glove_model[word] for word in sentence_str.split() if word in self.glove_model]\n",
    "        vecs = np.array(vecs)\n",
    "        return np.mean(vecs, axis=0)\n",
    "    \n",
    "    def train(self, X_train, X_train_feat, y_train):\n",
    "        train_vecs = []\n",
    "        for text in X_train:\n",
    "            train_vecs.append(self._get_sentence_embedding(text))\n",
    "        X_train = np.array(train_vecs)\n",
    "        scaler = StandardScaler()\n",
    "        X_feat_train = scaler.fit_transform(X_train_feat)\n",
    "        self.X_train = np.hstack((train_vecs, X_feat_train))\n",
    "        self.y_train = y_train\n",
    "        self.clf.fit(self.X_train, self.y_train)\n",
    "\n",
    "    def predict(self, X_test,X_test_feat):\n",
    "        test_vecs = []\n",
    "        for text in X_test:\n",
    "            test_vecs.append(self._get_sentence_embedding(text))\n",
    "        test_vecs  = np.array(test_vecs)\n",
    "        scaler = StandardScaler()\n",
    "        X_feat_test = scaler.fit_transform(X_test_feat)\n",
    "        X_test = np.hstack((test_vecs, X_feat_test))\n",
    "        return discretize(self.clf.predict(X_test))\n",
    "\n",
    "    def evaluate(self, X_test_text, X_test_feat, y_test):\n",
    "        y_pred = self.predict(X_test_text, X_test_feat)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "        recall = recall_score(y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')        \n",
    "        return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class GloVeSVM():\n",
    "    def __init__(self,  kernel='linear', C=1):\n",
    "        # Load the GloVe word vector model\n",
    "        self.glove_file='G:\\\\../L2/glove/glove.6B.300d.txt'\n",
    "        self.glove_model = self.load_glove_model(self.glove_file)\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.clf = SVC(kernel=self.kernel, C=self.C)\n",
    "\n",
    "    def load_glove_model(self, glove_file):\n",
    "        with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            glove_model = {}\n",
    "            for line in lines:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coef = np.asarray(values[1:], dtype='float32')\n",
    "                glove_model[word] = coef\n",
    "        return glove_model\n",
    "    \n",
    "    def _get_sentence_embedding(self, sentence):\n",
    "        sentence_str = ' '.join(sentence)\n",
    "        vecs = [self.glove_model[word] for word in sentence_str.split() if word in self.glove_model]\n",
    "        vecs = np.array(vecs)\n",
    "        return np.mean(vecs, axis=0)\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        train_vecs = []\n",
    "        for text in X_train:\n",
    "            train_vecs.append(self._get_sentence_embedding(text))\n",
    "        X_train = np.array(train_vecs)\n",
    "        y_train = np.array(y_train)\n",
    "        self.clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        test_vecs = []\n",
    "        for text in X_test:\n",
    "            test_vecs.append(self._get_sentence_embedding(text))\n",
    "        test_vecs  = np.array(test_vecs)\n",
    "        return discretize(self.clf.predict(test_vecs))\n",
    "\n",
    "    def evaluate(self, X_test_text, y_test):\n",
    "        y_pred = self.predict(X_test_text)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "        recall = recall_score(y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')        \n",
    "        return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import train_test_split\n",
    "def get_train_test(feature,label):\n",
    "    X_train, X_test, y_train, y_test=train_test_split(feature,label, test_size=0.2, random_state=42,stratify=label)\n",
    "    X_train=np.array(X_train)\n",
    "    X_test=np.array(X_test)\n",
    "    y_train=np.array(y_train)\n",
    "    y_train = y_train.ravel()\n",
    "    y_test=np.array(y_test)\n",
    "    y_test=y_test.ravel()\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g:\\\\广大资料\\\\跨指标难度评估-丁汉师兄\\\\数据文件\\\\code_Cross-corpus readability assessment compatibility for English texts\\\\model'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label-feature\n",
    "import pandas as pd\n",
    "############ CEFR #########################################\n",
    "CEFR = pd.read_csv(\"../L2/feature/[CEFR]with_features.csv\")\n",
    "df =pd.DataFrame(CEFR)\n",
    "label=df.iloc[:,1:2]\n",
    "feature=df.iloc[:,2:23]\n",
    "CEFR_feature_train, CEFR_feature_test, CEFR_featurey_train, CEFR_featurey_test=get_train_test(feature,label)\n",
    "# ############ CLEC #######################################################################################\n",
    "CLEC = pd.read_csv(\"../L2/feature/[CLEC]with_features.csv\")\n",
    "df =pd.DataFrame(CLEC)\n",
    "label=df.iloc[:,1:2]\n",
    "feature=df.iloc[:,2:23]\n",
    "CLEC_feature_train, CLEC_feature_test, CLEC_featurey_train, CLEC_featurey_test=get_train_test(feature,label)\n",
    "# ############ CLOTH #######################################################################################\n",
    "CLOTH = pd.read_csv(\"../L2/feature/[CLOTH]with_features.csv\")\n",
    "df =pd.DataFrame(CLOTH)\n",
    "label=df.iloc[:,1:2]\n",
    "feature=df.iloc[:,2:23]\n",
    "CLOTH_feature_train, CLOTH_feature_test, CLOTH_featurey_train, CLOTH_featurey_test=get_train_test(feature,label)\n",
    "# ############ NES #######################################################################################\n",
    "NES = pd.read_csv(\"../L2/feature/[NES]with_features.csv\")\n",
    "df =pd.DataFrame(NES)\n",
    "label=df.iloc[:,1:2]\n",
    "feature=df.iloc[:,2:23]\n",
    "NES_feature_train, NES_feature_test, NES_featurey_train, NES_featurey_test=get_train_test(feature,label)\n",
    "# ############ OSP #######################################################################################\n",
    "OSP = pd.read_csv(\"../L2/feature/[OSP]with_features.csv\")\n",
    "df =pd.DataFrame(OSP)\n",
    "label=df.iloc[:,1:2]\n",
    "feature=df.iloc[:,2:23]\n",
    "OSP_feature_train, OSP_feature_test, OSP_featurey_train, OSP_featurey_test=get_train_test(feature,label)\n",
    "# ############ RACE #######################################################################################\n",
    "RACE = pd.read_csv(\"../L2/feature/[RACE]with_features.csv\")\n",
    "df =pd.DataFrame(RACE)\n",
    "label=df.iloc[:,1:2]\n",
    "feature=df.iloc[:,2:23]\n",
    "RACE_feature_train, RACE_feature_test, RACE_featurey_train, RACE_featurey_test=get_train_test(feature,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label-text\n",
    "############ CEFR ##################################################################\n",
    "CEFR = pd.read_csv(\"../L2/feature/[CEFR]with_features.csv\")\n",
    "df =pd.DataFrame(CEFR)\n",
    "label=df.iloc[:,1:2]\n",
    "text=df.iloc[:,0:1]\n",
    "CEFR_X_train, CEFR_X_test, CEFR_y_train, CEFR_y_test=get_train_test(text,label)\n",
    "# ############ CLEC #################################################################\n",
    "CLEC = pd.read_csv(\"../L2/feature/[CLEC]with_features.csv\")\n",
    "df =pd.DataFrame(CLEC)\n",
    "label=df.iloc[:,1:2]\n",
    "text=df.iloc[:,0:1]\n",
    "CLEC_X_train, CLEC_X_test, CLEC_y_train, CLEC_y_test=get_train_test(text,label)\n",
    "# ############ CLOTH ################################################################\n",
    "CLOTH = pd.read_csv(\"../L2/feature/[CLOTH]with_features.csv\")\n",
    "df =pd.DataFrame(CLOTH)\n",
    "label=df.iloc[:,1:2]\n",
    "text=df.iloc[:,0:1]\n",
    "CLOTH_X_train, CLOTH_X_test, CLOTH_y_train, CLOTH_y_test=get_train_test(text,label)\n",
    "# ############ NES ##################################################################\n",
    "NES = pd.read_csv(\"../L2/feature/[NES]with_features.csv\")\n",
    "df =pd.DataFrame(NES)\n",
    "label=df.iloc[:,1:2]\n",
    "text=df.iloc[:,0:1]\n",
    "NES_X_train, NES_X_test, NES_y_train, NES_y_test=get_train_test(text,label)\n",
    "# ############ OSP ##################################################################\n",
    "OSP = pd.read_csv(\"../L2/feature/[OSP]with_features.csv\")\n",
    "df =pd.DataFrame(OSP)\n",
    "label=df.iloc[:,1:2]\n",
    "text=df.iloc[:,0:1]\n",
    "OSP_X_train, OSP_X_test, OSP_y_train, OSP_y_test=get_train_test(text,label)\n",
    "# ############ RACE #################################################################\n",
    "RACE = pd.read_csv(\"../L2/feature/[RACE]with_features.csv\")\n",
    "df =pd.DataFrame(RACE)\n",
    "label=df.iloc[:,1:2]\n",
    "text=df.iloc[:,0:1]\n",
    "RACE_X_train, RACE_X_test, RACE_y_train, RACE_y_test=get_train_test(text,label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost+glove+feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'CEFR': [CEFR_X_train,CEFR_y_train,CEFR_X_test,CEFR_y_test,CEFR_feature_train, CEFR_feature_test],\n",
    "            'CLEC': [CLEC_X_train,CLEC_y_train,CLEC_X_test,CLEC_y_test,CLEC_feature_train, CLEC_feature_test],\n",
    "            'CLOTH': [CLOTH_X_train,CLOTH_y_train,CLOTH_X_test,CLOTH_y_test,CLOTH_feature_train, CLOTH_feature_test],\n",
    "            'NES': [NES_X_train,NES_y_train,NES_X_test,NES_y_test,NES_feature_train, NES_feature_test],\n",
    "            'OSP': [OSP_X_train,OSP_y_train,OSP_X_test,OSP_y_test,OSP_feature_train, OSP_feature_test],\n",
    "            'RACE': [RACE_X_train,RACE_y_train,RACE_X_test,RACE_y_test,RACE_feature_train, RACE_feature_test]}\n",
    "\n",
    "all_res = []\n",
    "for data in datasets:\n",
    "    X_train, y_train, X_test, y_test ,feature_train,feature_test= datasets[data]\n",
    "    model = GloVeFeatXGBoost()\n",
    "    model.train(X_train,feature_train, y_train)\n",
    "    accuracy, precision, recall, f1 = model.evaluate(X_test,feature_test, y_test)\n",
    "    res = [data, accuracy, precision, recall, f1]\n",
    "    all_res.append(res)\n",
    "print(all_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetname=['CEFR','CLEC','CLOTH','NES','OSP','RACE']\n",
    "trainfeat=[[CEFR_X_train,CEFR_feature_train,CEFR_y_train],\n",
    "       [CLEC_X_train,CLEC_feature_train,CLEC_y_train],\n",
    "       [CLOTH_X_train,CLOTH_feature_train,CLOTH_y_train],\n",
    "       [NES_X_train,NES_feature_train,NES_y_train],\n",
    "       [OSP_X_train,OSP_feature_train,OSP_y_train]]\n",
    "testsfeat=[[CEFR_X_test,CEFR_feature_test,CEFR_y_test],\n",
    "       [CLEC_X_test,CLEC_feature_test,CLEC_y_test],\n",
    "       [CLOTH_X_test,CLOTH_feature_test,CLOTH_y_test],\n",
    "       [NES_X_test,NES_feature_test,NES_y_test],\n",
    "       [OSP_X_test,OSP_feature_test,OSP_y_test]]\n",
    "\n",
    "i=-1\n",
    "for X_train, X_train_feat, y_train in trainfeat:\n",
    "    i=i+1\n",
    "    j=-1\n",
    "    model = GloVeFeatXGBoost()\n",
    "    model.train(X_train, X_train_feat, y_train)\n",
    "    cor_metric = []\n",
    "    result = {}\n",
    "    for X_test,X_test_feat,y_test in testsfeat:\n",
    "        j=j+1\n",
    "        y_pred = model.predict(X_test, X_test_feat)\n",
    "        result['Numy'] = y_test\n",
    "        result['Numyp'] = y_pred\n",
    "        df = pd.DataFrame(result)\n",
    "        cor=print_metrics(y_test, y_pred)\n",
    "        cor_metric.append(cor)\n",
    "        df.to_csv('G:\\\\ML_result\\\\ML+glove+feature\\\\'+datasetname[i]+'_'+datasetname[j]+'_xgboost.txt',header=False, index=False,sep=' ')\n",
    "    cor = pd.DataFrame(columns=['NDCG','RJSD','RRNSS'], data=cor_metric)\n",
    "    cor.to_csv('G:\\\\ML_result\\\\ML+glove+feature\\\\cor\\\\cor_xgboost_'+datasetname[i]+'.csv')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost+glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = []\n",
    "for data in datasets:\n",
    "    X_train, y_train, X_test, y_test ,feature_train,feature_test= datasets[data]\n",
    "    model = GloVeXGBoost()\n",
    "    model.train(X_train, y_train)\n",
    "    accuracy, precision, recall, f1 = model.evaluate(X_test, y_test)\n",
    "    res = [data, accuracy, precision, recall, f1]\n",
    "    all_res.append(res)\n",
    "print(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=[[CEFR_X_train,CEFR_y_train],[CLEC_X_train,CLEC_y_train],[CLOTH_X_train,CLOTH_y_train],[NES_X_train,NES_y_train],[OSP_X_train,OSP_y_train],[RACE_X_train,RACE_y_train]]\n",
    "tests=[[CEFR_X_test,CEFR_y_test],[CLEC_X_test,CLEC_y_test],[CLOTH_X_test,CLOTH_y_test],[NES_X_test,NES_y_test],[OSP_X_test,OSP_y_test],[RACE_X_test,RACE_y_test]]\n",
    "\n",
    "i=-1\n",
    "for X_train, y_train in train:\n",
    "    i=i+1\n",
    "    j=-1\n",
    "    model = GloVeXGBoost()\n",
    "    model.train(X_train, y_train)\n",
    "    cor_metric = []\n",
    "    result = {}\n",
    "    for X_test,y_test in tests:\n",
    "        j=j+1\n",
    "        y_pred = model.predict(X_test)\n",
    "        result['Numy'] = y_test\n",
    "        result['Numyp'] = y_pred\n",
    "        df = pd.DataFrame(result)\n",
    "        cor=print_metrics(y_test, y_pred)\n",
    "        cor_metric.append(cor)\n",
    "        df.to_csv('G:\\\\ML_result\\\\ML+glove-new\\\\'+datasetname[i]+'_'+datasetname[j]+'_xgboost.txt',header=False, index=False,sep=' ')\n",
    "    cor = pd.DataFrame(columns=['NDCG','RJSD','RRNSS'], data=cor_metric)\n",
    "    cor.to_csv('G:\\\\ML_result\\\\ML+glove-new\\\\cor\\\\cor_xgboost_'+datasetname[i]+'.csv')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM+glove+feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = []\n",
    "for data in datasets:\n",
    "    X_train, y_train, X_test, y_test ,feature_train,feature_test= datasets[data]\n",
    "    model = GloVeFeatSVM()\n",
    "    model.train(X_train,feature_train, y_train)\n",
    "    accuracy, precision, recall, f1 = model.evaluate(X_test,feature_test, y_test)\n",
    "    res = [data, accuracy, precision, recall, f1]\n",
    "    all_res.append(res)\n",
    "\n",
    "print(all_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=-1\n",
    "for X_train, X_train_feat, y_train in trainfeat:\n",
    "    i=i+1\n",
    "    j=-1\n",
    "    model = GloVeFeatSVM()\n",
    "    model.train(X_train, X_train_feat, y_train)\n",
    "    cor_metric = []\n",
    "    result = {}\n",
    "    for X_test,X_test_feat,y_test in testsfeat:\n",
    "        j=j+1\n",
    "        y_pred = model.predict(X_test, X_test_feat)\n",
    "        result['Numy'] = y_test\n",
    "        result['Numyp'] = y_pred\n",
    "        df = pd.DataFrame(result)\n",
    "        cor=print_metrics(y_test, y_pred)\n",
    "        cor_metric.append(cor)\n",
    "        df.to_csv('G:\\\\ML_result\\\\ML+glove+feature\\\\'+datasetname[i]+'_'+datasetname[j]+'_SVM.txt',header=False, index=False,sep=' ')\n",
    "    cor = pd.DataFrame(columns=['NDCG','RJSD','RRNSS'], data=cor_metric)\n",
    "    cor.to_csv('G:\\\\ML_result\\\\ML+glove+feature\\\\cor\\\\cor_SVM_'+datasetname[i]+'.csv')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM+glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = []\n",
    "for data in datasets:\n",
    "    X_train, y_train, X_test, y_test ,feature_train,feature_test= datasets[data]\n",
    "    model = GloVeSVM()\n",
    "    model.train(X_train, y_train)\n",
    "    accuracy, precision, recall, f1 = model.evaluate(X_test, y_test)\n",
    "    res = [data, accuracy, precision, recall, f1]\n",
    "    all_res.append(res)\n",
    "print(all_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=-1\n",
    "for X_train,  y_train in train:\n",
    "    i=i+1\n",
    "    j=-1\n",
    "    model = GloVeSVM()\n",
    "    model.train(X_train,  y_train)\n",
    "    cor_metric = []\n",
    "    result = {}\n",
    "    for X_test,y_test in tests:\n",
    "        j=j+1\n",
    "        y_pred = model.predict(X_test)\n",
    "        result['Numy'] = y_test\n",
    "        result['Numyp'] = y_pred\n",
    "        df = pd.DataFrame(result)\n",
    "        cor=print_metrics(y_test, y_pred)\n",
    "        cor_metric.append(cor)\n",
    "        df.to_csv('G:\\\\ML_result\\\\ML+glove+feature\\\\'+datasetname[i]+'_'+datasetname[j]+'_SVM.txt',header=False, index=False,sep=' ')\n",
    "    cor = pd.DataFrame(columns=['NDCG','RJSD','RRNSS'], data=cor_metric)\n",
    "    cor.to_csv('G:\\\\ML_result\\\\ML+glove+feature\\\\cor\\\\cor_SVM_'+datasetname[i]+'.csv')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
