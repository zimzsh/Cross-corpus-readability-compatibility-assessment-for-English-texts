{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract text and labels from the original dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RACE\n",
    "Method 1: Extracting txt and label from the dict data type.\n",
    "\n",
    "Method 2: The dataset includes reading comprehension sections from middle school and high school exams, referred to as RC. https://huggingface.co/datasets/race\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file: 13523.txt (list index out of range)\n",
      "Error processing file: 15577.txt (list index out of range)\n",
      "Error processing file: 1582.txt (list index out of range)\n",
      "Error processing file: 18583.txt (list index out of range)\n",
      "Error processing file: 21211.txt (list index out of range)\n",
      "Error processing file: 21566.txt (list index out of range)\n",
      "Error processing file: 23959.txt (list index out of range)\n",
      "Error processing file: 4072.txt (list index out of range)\n",
      "Error processing file: 4228.txt (list index out of range)\n",
      "Error processing file: 6391.txt (list index out of range)\n",
      "Error processing file: 8335.txt (list index out of range)\n",
      "Error processing file: 15561.txt (list index out of range)\n",
      "Error processing file: 17429.txt (list index out of range)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "def tokenize(st):\n",
    "    ans = []\n",
    "    for sent in sent_tokenize(st):\n",
    "        ans += word_tokenize(sent)\n",
    "    return \" \".join(ans).lower()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    difficulty_set = [\"middle\", \"high\"]\n",
    "    # 首先修改 data、raw_data、csv_filename对应路径\n",
    "    data = r\"D:\\00_HanLife\\成果整理\\数据整理\\B_L2_en\\01_RACE\"\n",
    "    raw_data = r\"D:\\00_HanLife\\成果整理\\数据整理\\B_L2_en\\01_RACE\\RACE\"\n",
    "    \n",
    "    csv_filename = r\"D:\\00_HanLife\\成果整理\\数据整理\\B_L2_en\\01_RACE\\race.csv\"  # Specify the filename to save the CSV data, and make sure to modify it to the designated file.\n",
    "    \n",
    "    with open(csv_filename, mode='w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"Article\", \"Label\"])  \n",
    "        \n",
    "        for data_set in [\"train\", \"dev\", \"test\"]:\n",
    "            for d in difficulty_set:\n",
    "                new_data_path = os.path.join(data, data_set, d)\n",
    "                new_raw_data_path = os.path.join(raw_data, data_set, d)\n",
    "                \n",
    "                for inf in os.listdir(new_raw_data_path):\n",
    "                    try:\n",
    "                        obj = json.load(open(os.path.join(new_raw_data_path, inf), \"r\"))\n",
    "                        article = tokenize(obj[\"article\"])\n",
    "                        label = d\n",
    "                        \n",
    "                        writer.writerow([article, label])  \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file: {inf} ({e})\")\n",
    "                        continue\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLOTH\n",
    "将dict类型数据中提取txt和label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file: high1209.json (list index out of range)\n",
      "Error processing file: high381.json (list index out of range)\n",
      "Error processing file: high586.json (list index out of range)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "def tokenize(st):\n",
    "    # TODO: The tokenizer's performance is suboptimal\n",
    "    ans = []\n",
    "    for sent in sent_tokenize(st):\n",
    "        ans += word_tokenize(sent)\n",
    "    return \" \".join(ans).lower()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    difficulty_set = [\"middle\", \"high\"]\n",
    "    # 首先修改 data、raw_data、csv_filename对应路径\n",
    "    data = r\"D:\\00_HanLife\\成果整理\\数据整理\\B_L2_en\\02_CLOTH\"\n",
    "    raw_data = r\"D:\\00_HanLife\\成果整理\\数据整理\\B_L2_en\\02_CLOTH\\CLOTH\"\n",
    "    \n",
    "    csv_filename = r\"D:\\00_HanLife\\成果整理\\数据整理\\B_L2_en\\02_CLOTH\\cloth.csv\"  \n",
    "    \n",
    "    with open(csv_filename, mode='w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"Article\", \"Label\"])  \n",
    "        \n",
    "        for data_set in [\"train\", \"valid\", \"test\"]:\n",
    "            for d in difficulty_set:\n",
    "                new_data_path = os.path.join(data, data_set, d)\n",
    "                new_raw_data_path = os.path.join(raw_data, data_set, d)\n",
    "                \n",
    "                for inf in os.listdir(new_raw_data_path):\n",
    "                    try:\n",
    "                        obj = json.load(open(os.path.join(new_raw_data_path, inf), \"r\"))\n",
    "                        article = tokenize(obj[\"article\"])\n",
    "                        label = d\n",
    "                        \n",
    "                        writer.writerow([article, label])  \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file: {inf} ({e})\")\n",
    "                        continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLEC\n",
    "The dataset includes five levels: ST2, ST3, ST4, ST5, and ST6. However, all the textual content is present in a single txt file, which contains a large amount of irrelevant information and annotations.\n",
    "\n",
    "By observing the txt file, we can see that each text segment consists of an article title and its corresponding content. We can analyze specific articles by identifying the special identifiers used for article titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "#from nltk.corpus import stopwords\n",
    "#stopset = set(stopwords.words('english'))\n",
    "\n",
    "def file_max_num(save_path,level):\n",
    "    count = []\n",
    "    for s in os.listdir(os.path.join(save_path,level)):\n",
    "        #print(s)\n",
    "        name = str(os.path.basename(s))\n",
    "        name = name.replace(str(level),'')\n",
    "        name = name.replace('_','')\n",
    "        name = name.replace('.txt','')\n",
    "        count.append(int(name))\n",
    "    return max(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#CLEC:raw ----- Standard\n",
    "path = r\"C:\\Users\\54554\\Desktop\\clec\"\n",
    "save_path = r\"C:\\Users\\54554\\PycharmProjects\\race_stage1\\data\\posted_data\\all_posted_data2\"\n",
    "for o in os.listdir(path):\n",
    "    open_path = os.path.join(path,o)\n",
    "    if \"2\" in o:\n",
    "        with open(open_path,encoding=\"utf-8\") as f:\n",
    "            data = f.readlines()\n",
    "        for d in data:\n",
    "            if \"<ST\" in d:\n",
    "                data.remove(d)\n",
    "            elif \"\\n\" == d or \" \\n\" == d or \"         \\n\" == d:\n",
    "                data.remove(d)\n",
    "        count = 0\n",
    "        for d in data:\n",
    "            if len(d.split()) >= 70:\n",
    "                count = count + 1\n",
    "                with open(save_path +\"\\\\ST2\\\\\" + str(count) +'.txt',\"w\",encoding=\"utf-8\") as f:\n",
    "                    f.write(d)# ST2\n",
    "    elif \"3\" in o:\n",
    "        with open(open_path,encoding=\"utf-8\") as f:\n",
    "            data = f.readlines()\n",
    "        for d in data:\n",
    "            if len(d.split()) < 200:\n",
    "                data.remove(d)\n",
    "            elif \"\\n\" == d:\n",
    "                data.remove(d)\n",
    "        count = 0\n",
    "        for d in data:\n",
    "            if len(d.split()) >= 70:\n",
    "                count = count + 1\n",
    "                with open(save_path +\"\\\\ST3\\\\\" + str(count) +'.txt',\"w\",encoding=\"utf-8\") as f:\n",
    "                    f.write(d)#ST3\n",
    "    elif \"4\" in o:\n",
    "        with open(open_path,encoding=\"utf-8\") as f:\n",
    "            data = f.readlines()\n",
    "        count = 0\n",
    "        for d in data:\n",
    "            if len(d) >= 140:\n",
    "                count = count + 1\n",
    "                with open(save_path +\"\\\\ST4\\\\\" + str(count) +'.txt',\"w\",encoding=\"utf-8\") as f:\n",
    "                     f.write(d)#ST4\n",
    "    elif \"5\" in o:\n",
    "        with open(open_path,encoding=\"utf-8\") as f:\n",
    "            data = f.readlines()\n",
    "        count = 0\n",
    "        for d in data:\n",
    "            if len(d) >= 140:\n",
    "                count = count + 1\n",
    "                with open(save_path +\"\\\\ST5\\\\\" + str(count) +'.txt',\"w\",encoding=\"utf-8\") as f:\n",
    "                     f.write(d)#ST5\n",
    "    else:\n",
    "        with open(open_path,encoding=\"utf-8\") as f:\n",
    "            data = f.readlines()\n",
    "        count = 0\n",
    "        data_ok = []\n",
    "        for d in data:\n",
    "            if \"<ST\" in d and len(d.split(\" \")) < 50:\n",
    "                #data.remove(d)\n",
    "                continue\n",
    "            elif \"\\n\" == d or \" \\n\" == d or \"         \\n\" == d:\n",
    "                #data.remove(d)\n",
    "                continue\n",
    "            else:data_ok.append(d)\n",
    "        for d in data_ok:\n",
    "            if len(d.split()) > 2:\n",
    "                count = count + 1\n",
    "                with open(save_path +\"\\\\ST6\\\\\" + str(count) +'.txt',\"w\",encoding=\"utf-8\") as f:\n",
    "                    f.write(d)#ST6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def clean_space(text):\n",
    "    \"\"\"\"\n",
    "    Remove excess whitespace\n",
    "    \"\"\"\n",
    "    match_regex = re.compile(u'[\\u4e00-\\u9fa5。\\.,，:：《》、\\(\\)（）]{1} +(?<![a-zA-Z])|\\d+ +| +\\d+|[a-z A-Z]+')\n",
    "    should_replace_list = match_regex.findall(text)\n",
    "    order_replace_list = sorted(should_replace_list,key=lambda i:len(i),reverse=True)\n",
    "    for i in order_replace_list:\n",
    "        if i == u' ':\n",
    "            continue\n",
    "        new_i = i.strip()\n",
    "        text = text.replace(i,new_i)\n",
    "    return text\n",
    "\n",
    "data = []\n",
    "for f1 in os.listdir(save_path):\n",
    "    #print(f)\n",
    "    level = f1\n",
    "    path1 = os.path.join(save_path,f1)\n",
    "    for f2 in os.listdir(path1):\n",
    "        #print(f2)\n",
    "        path2 = os.path.join(path1,f2)        \n",
    "        with open(path2,encoding=\"utf-8\") as f:\n",
    "            raw_txt = f.read()\n",
    "        raw_txt = raw_txt.replace('\\n', '')\n",
    "        #------------------------------------------------------\n",
    "        \n",
    "        d = raw_txt\n",
    "        string = \" \".join(s.strip() for s in d.split() if \"<\" not in s and \">\" not in s and \"[\" not in s and ']' not in s)\n",
    "        string = clean_space(string)\n",
    "        #------------------------------------------------------\n",
    "        data.append([string,level])\n",
    "        \n",
    "name = [\"article\",\"level\"]\n",
    "df_clec = pd.DataFrame(data=data,index=None,columns=name)\n",
    "df_clec.to_csv(\"./data/clec/data_clec.csv\",index = None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSP\n",
    "The dataset is well organized into three difficulty levels (Adv, Ele, Int) corresponding to different folders. Each folder contains specific articles, and there are also comparative texts available for each difficulty level. It is convenient to directly utilize this dataset from HuggingFace.\n",
    "link：https://huggingface.co/datasets/onestop_english\n",
    "code：\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"onestop_english\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NES\n",
    "This data requires a web scraper for preprocessing the text. Please refer to the following link for more information: https://github.com/shlomihod/deep-text-eval/blob/master/data/newsela/prepare_newsela.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "\n",
    "HTML_TAG_RE = re.compile('<.*?>', re.DOTALL)\n",
    "MARKDOWN_IMAGE_RE = re.compile('!{0,1}\\[.*?\\]\\s?\\(.*?\\)', re.DOTALL)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "MIN_N_CATEGORY_DATA = 30\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "BASE_URL = 'https://newsela.com/api/v2/articleheader/'\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "\n",
    "class Not200Error(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_newsela_api(part):\n",
    "    r = requests.get(BASE_URL + part, headers=HEADERS)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        raise Not200Error\n",
    "\n",
    "    return r\n",
    "\n",
    "\n",
    "def get_group_df():\n",
    "    page_jsons = []\n",
    "\n",
    "    for page_number in tqdm(itertools.count(start=1)):\n",
    "\n",
    "        result = get_newsela_api('?page={}'.format(page_number)).json()\n",
    "\n",
    "        if not result:\n",
    "            break\n",
    "\n",
    "        page_jsons.append(result)\n",
    "\n",
    "    group_df = pd.concat(map(pd.DataFrame, page_jsons), sort=False)\n",
    "\n",
    "    return group_df\n",
    "\n",
    "\n",
    "def slug2articles(slug):\n",
    "    r = get_newsela_api(slug)\n",
    "\n",
    "    return (pd.DataFrame(\n",
    "        r.json()['articles'])\n",
    "      .assign(slug=slug))\n",
    "\n",
    "\n",
    "def get_text_df(slugs):\n",
    "    articles = []\n",
    "    errors_indices = []\n",
    "\n",
    "    for index, slug in enumerate(tqdm(slugs)):\n",
    "        try:\n",
    "            articles.append(slug2articles(slug))\n",
    "        except Not200Error:\n",
    "            errors_indices.append(index)\n",
    "\n",
    "\n",
    "    text_df = pd.concat(articles)\n",
    "\n",
    "    return text_df, errors_indices\n",
    "\n",
    "def remove_too_small_categories(text_df):\n",
    "    category_counts = text_df['y'].value_counts()\n",
    "    categories_to_keep = category_counts.index[category_counts >= MIN_N_CATEGORY_DATA]\n",
    "\n",
    "    print('Removed Data:')\n",
    "    pprint(text_df[\n",
    "            ~text_df['y'].isin(categories_to_keep)\n",
    "        ]['y'].value_counts())\n",
    "\n",
    "    text_df = text_df[text_df['y'].isin(categories_to_keep)]\n",
    "    return text_df\n",
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    return HTML_TAG_RE.sub(' ', text)\n",
    "\n",
    "\n",
    "def remove_markdown_image_tags(text):\n",
    "    return MARKDOWN_IMAGE_RE.sub(' ', text)\n",
    "\n",
    "\n",
    "def print_data_stats(text_df, train_df, test_df):\n",
    "    dfs = [text_df, train_df, test_df]\n",
    "\n",
    "    text_counts = text_df['y'].value_counts()\n",
    "    train_counts = train_df['y'].value_counts()\n",
    "    test_counts = test_df['y'].value_counts()\n",
    "\n",
    "    print('Categorial y')\n",
    "    print(pd.DataFrame({\n",
    "        '#text': text_counts,\n",
    "        '%text': (100 * text_counts / text_counts.sum()).round(2),\n",
    "        '#train': train_counts,\n",
    "        '%train': (100 * train_counts / train_counts.sum()).round(2),\n",
    "        '#test': test_counts,\n",
    "        '%test': (100 * test_counts / test_counts.sum()).round(2),\n",
    "    }).sort_index())\n",
    "\n",
    "    print()\n",
    "\n",
    "    stats_d = {'name': ['text', 'train', 'test'],\n",
    "             '#': [len(df['y_lexile']) for df in dfs],\n",
    "             'min': [df['y_lexile'].min() for df in dfs],\n",
    "             'max': [df['y_lexile'].max() for df in dfs],\n",
    "             'mean': [df['y_lexile'].mean() for df in dfs],\n",
    "             'std': [df['y_lexile'].std() for df in dfs]\n",
    "            }\n",
    "\n",
    "    stats_df = pd.DataFrame(stats_d)\n",
    "    stats_df.set_index('name')\n",
    "\n",
    "    print('Continuous y_lexile')\n",
    "    pprint(stats_df)\n",
    "    print()\n",
    "    print('train-test Kolmogorov-Smirnov p-value', stats.ks_2samp(train_df['y_lexile'],\n",
    "                                                               test_df['y_lexile']).pvalue)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_corpus():\n",
    "\n",
    "    print('Getting slugs...')\n",
    "    group_df = get_group_df()\n",
    "\n",
    "    print('Filterig non english slugs...')\n",
    "    print(group_df['language'].value_counts())\n",
    "    group_df = group_df[group_df['language'] == 'en']\n",
    "    print(group_df['language'].value_counts())\n",
    "\n",
    "    # it makes a problem to the parser\n",
    "    # https://github.com/nikitakit/self-attentive-parser/issues/5\n",
    "    print('Removing US-Constitution Text...')\n",
    "    group_df = group_df[group_df['slug'] != 'US-Constitution']\n",
    "    print('#Groups =', len(group_df))\n",
    "\n",
    "\n",
    "    print('Getting Texts...')\n",
    "    text_df, errors_indices = get_text_df(group_df['slug'])\n",
    "\n",
    "    if errors_indices:\n",
    "        print('Error Indices:', errors_indices)\n",
    "\n",
    "        print('Retrying Getting Texts...')\n",
    "        retry_text_df, retry_errors_indices = get_text_df(group_df.loc[errors_indices, 'slug'])\n",
    "\n",
    "        print('Error Indices:', retry_errors_indices)\n",
    "\n",
    "        print('Concating Texts...')\n",
    "        text_df = pd.concat([text_df, retry_text_df])\n",
    "\n",
    "    text_df = text_df.rename({'lexile_level': 'y_lexile', 'grade_level': 'y'}, axis=1)\n",
    "    text_df['y'] = text_df['y'].astype(int)\n",
    "    print('#Texts =', len(text_df))\n",
    "\n",
    "    print('Removing Too Small Categories...')\n",
    "    text_df = remove_too_small_categories(text_df)\n",
    "    print('#Texts =', len(text_df))\n",
    "\n",
    "    print('Removing HTML Tags, Markdown Image Tags, and Extra Dashes...')\n",
    "    text_df['text'] = (text_df['text']\n",
    "         .apply(remove_html_tags)\n",
    "         .apply(remove_markdown_image_tags).str.replace('----', ' '))\n",
    "\n",
    "    print('Reset Index...')\n",
    "    text_df = text_df.reset_index(drop=True)\n",
    "\n",
    "    print('Train-Test Split...')\n",
    "    train_df, test_df = train_test_split(text_df,\n",
    "                                     test_size=TEST_SIZE,\n",
    "                                     shuffle=True,\n",
    "                                     stratify=text_df['y'],\n",
    "                                     random_state=RANDOM_STATE)\n",
    "\n",
    "    print_data_stats(text_df, train_df, test_df)\n",
    "    return text_df, train_df, test_df, text_df, group_df\n",
    "\n",
    "\n",
    "print('Getting slugs...')\n",
    "group_df = get_group_df()\n",
    "print('Filterig non english slugs...')\n",
    "print(group_df['language'].value_counts())\n",
    "group_df = group_df[group_df['language'] == 'en']\n",
    "print(group_df['language'].value_counts())\n",
    "print('Removing US-Constitution Text...')\n",
    "group_df = group_df[group_df['slug'] != 'US-Constitution']\n",
    "print('#Groups =', len(group_df))\n",
    "\n",
    "\n",
    "print('Getting Texts...')\n",
    "text_df, errors_indices = get_text_df(group_df['slug'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CEFR\n",
    "This dataset has five difficulty levels, ranging from A1 to C2. The original dataset is incomplete.\n",
    "\n",
    "Processed data can be found at: https://github.com/akkikiki/diff_joint_estimate/tree/master/data/processed\n",
    "\n",
    "Code for loading the dataset is available at: https://github.com/akkikiki/diff_joint_estimate/blob/master/src/data/datasets.py\n",
    "\n",
    "To summarize, the first step is to obtain the dataset by following the provided links.\n",
    "\n",
    "!wget https://s3-eu-west-1.amazonaws.com/ilexir-website-media/Readability_dataset.tar.gz\n",
    "\n",
    "!tar xvf Readability_dataset.tar.gz -C data/raw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Tuple, List\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import List, Optional\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "class DatasetSplit(Enum):\n",
    "    TRAIN = 'train'\n",
    "    DEV = 'dev'\n",
    "    TEST = 'test'\n",
    "    UNLABELED = 'unlabeled'\n",
    "\n",
    "class Word:\n",
    "    word_str: str\n",
    "    label: Optional[str]\n",
    "    docs: List[str]\n",
    "    split: DatasetSplit\n",
    "    word_id: Optional[int] = None\n",
    "    doc_ids: Optional[List[int]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    doc_str: str\n",
    "    label: Optional[str]\n",
    "    text: str\n",
    "    words: List[str]\n",
    "    split: DatasetSplit\n",
    "    doc_id: Optional[int] = None\n",
    "    word_ids: Optional[List[int]] = None\n",
    "\n",
    "UK2US_WORDS = {}\n",
    "def normalize_tokens(tokens: List[str]) -> List[str]:\n",
    "    tokens_normalized = []\n",
    "    for token in tokens:\n",
    "        if not re.match(r'[A-Za-z]+', token):\n",
    "            continue\n",
    "        token = token.lower()\n",
    "        token = UK2US_WORDS.get(token, token)\n",
    "\n",
    "        tokens_normalized.append(token)\n",
    "\n",
    "    return tokens_normalized\n",
    "\n",
    "import hashlib\n",
    "\n",
    "EXAM2CEFR = {\n",
    "    \"CAE\": \"C1\",\n",
    "    \"CPE\": \"C2\",\n",
    "    \"PET\": \"B1\",\n",
    "    \"FCE\": \"B2\",\n",
    "    \"KET\": \"A2\"\n",
    "}\n",
    "\n",
    "\n",
    "def get_dataset_split(string_id: str, training_portion: int = 10) -> DatasetSplit:\n",
    "    \"\"\"Given some string ID, return the dataset split (train/dev/test)\"\"\"\n",
    "    if not 1 <= training_portion <= 10:\n",
    "        raise ValueError(f'Invalid training portion: {training_portion}')\n",
    "\n",
    "    md5_hex = hashlib.md5(string_id.encode('utf-8')).hexdigest()\n",
    "    hashed = int(md5_hex, 16) % 20\n",
    "    if hashed < 3:\n",
    "        return DatasetSplit.TEST\n",
    "    elif hashed < 6:\n",
    "        return DatasetSplit.DEV\n",
    "    else:\n",
    "        if training_portion == 10:\n",
    "            return DatasetSplit.TRAIN\n",
    "\n",
    "        training_md5 = hashlib.md5(('salt'+string_id).encode('utf-8')).hexdigest()\n",
    "        training_hashed = int(training_md5, 16) % 10\n",
    "        if training_hashed < training_portion:\n",
    "            return DatasetSplit.TRAIN\n",
    "        else:\n",
    "            return DatasetSplit.UNLABELED\n",
    "\n",
    "\n",
    "def read_cefrj_wordlist(training_portion: int = 10) -> Iterable[Word]:\n",
    "    # TODO: deal with polysemy\n",
    "    with open('data/processed/cefrj_wordlist/all.csv', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            cefr = row['CEFR']\n",
    "            word_str = row['headword']\n",
    "            # if there are multiple spellings, pick the first one\n",
    "            if '/' in word_str:\n",
    "                word_str = word_str.split('/')[0]\n",
    "\n",
    "            word = Word(\n",
    "                word_str=word_str,\n",
    "                label=cefr,\n",
    "                docs=[],\n",
    "                split=get_dataset_split(f'cefrj:{word_str}',\n",
    "                                        training_portion=training_portion)\n",
    "            )\n",
    "            yield word\n",
    "\n",
    "\n",
    "def split_document_to_paragraphs(filename: str) \\\n",
    "        -> Iterable[Tuple[int, str, List[str]]]:\n",
    "    \"\"\"Read a document from filename, and yield tuples\n",
    "        (paragraph_no, paragraph, tokens), where\n",
    "        paragraph_no (int): the 0-base paragraph number,\n",
    "        paragraph (str): the paragraph string, and\n",
    "        tokens (list of strs): tokenized sentence\"\"\"\n",
    "    with open(filename) as f:\n",
    "        lines = f.read()\n",
    "    paragraphs = lines.split(\"\\n\\n\")\n",
    "\n",
    "    for para_id, paragraph in enumerate(paragraphs):\n",
    "        if len(paragraph) < 64:\n",
    "            continue\n",
    "        tokens = normalize_tokens(word_tokenize(paragraph))\n",
    "        yield para_id, paragraph, tokens\n",
    "\n",
    "\n",
    "def read_cambridge_readability_dataset(training_portion: int = 10) -> Iterable[Document]:\n",
    "    ROOT_DIR = \"data/raw/Readability_dataset/\"\n",
    "    HEADERS = set([\"A.\", \"B.\", \"C.\"])\n",
    "\n",
    "    for filename in Path(ROOT_DIR).glob('**/*.txt'):\n",
    "        exam_name = os.path.basename(os.path.dirname(filename))\n",
    "        file_id = os.path.basename(filename).split('.')[0]\n",
    "        cefr = EXAM2CEFR[exam_name]\n",
    "\n",
    "        for para_id, paragraph, tokens in split_document_to_paragraphs(filename):\n",
    "            if paragraph in HEADERS:\n",
    "                continue\n",
    "\n",
    "            doc_str = f'{exam_name}-{file_id}-{para_id}'\n",
    "\n",
    "            doc = Document(\n",
    "                doc_str=doc_str,\n",
    "                label=cefr,\n",
    "                text=paragraph,\n",
    "                words=tokens,\n",
    "                split=get_dataset_split(f'crd:{doc_str}',\n",
    "                                        training_portion=training_portion)\n",
    "            )\n",
    "            yield doc\n",
    "\n",
    "\n",
    "def read_a1_passages(training_portion: int = 10) -> Iterable[Document]:\n",
    "    ROOT_DIR = 'data/processed/a1_passages/'\n",
    "\n",
    "    for filename in Path(ROOT_DIR).glob('*.txt'):\n",
    "        file_id = os.path.basename(filename).split('.')[0]\n",
    "\n",
    "        for paragraph_no, paragraph, tokens in split_document_to_paragraphs(filename):\n",
    "            doc_str = f'A1-{file_id}-{paragraph_no}'\n",
    "            doc = Document(\n",
    "                doc_str=doc_str,\n",
    "                label='A1',\n",
    "                text=paragraph,\n",
    "                words=tokens,\n",
    "                split=get_dataset_split(f'a1:{doc_str}',\n",
    "                                        training_portion=training_portion)\n",
    "            )\n",
    "            yield doc\n",
    "\n",
    "\n",
    "def read_efcamdat_dataset(efcamdat_file_path: str, sent_threshold: int = 2) -> Iterable[Document]:\n",
    "    with open(efcamdat_file_path) as f:\n",
    "        for line_no, line in enumerate(f):\n",
    "            if line_no % 10000 == 0:\n",
    "                print(f'Processed {line_no} lines...', file=sys.stderr)\n",
    "\n",
    "            data = json.loads(line)\n",
    "            essay_id = int(data['id'])\n",
    "\n",
    "            text = data.get('corrected_text')\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            for sent_no, sentence in enumerate(sent_tokenize(text)):\n",
    "                if sent_no > sent_threshold:\n",
    "                    break\n",
    "                tokens = normalize_tokens(word_tokenize(sentence))\n",
    "                doc = Document(\n",
    "                    doc_str=f'efcamdat{essay_id}-{sent_no}',\n",
    "                    label=None,     # unlabeled instance\n",
    "                    text=sentence,\n",
    "                    words=tokens,\n",
    "                    split=DatasetSplit.UNLABELED\n",
    "                )\n",
    "                yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--training-portion', type=int, default=10,\n",
    "                    help='Specify the amount of training data between 1 (10%) and 10 (100%).')\n",
    "args = parser.parse_args()\n",
    "docs = list(itertools.chain(\n",
    "        read_cambridge_readability_dataset(training_portion=args.training_portion),\n",
    "        read_a1_passages(training_portion=args.training_portion)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eduRB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
