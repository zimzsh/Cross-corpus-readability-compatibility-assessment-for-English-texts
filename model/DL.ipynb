{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"HxxjHi5DpMlR"},"source":["Datasets"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7091,"status":"ok","timestamp":1682165920518,"user":{"displayName":"zhiqin li","userId":"14648237152869568839"},"user_tz":-480},"id":"l2ozbvbRpD_E"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","# text label\n","########## CEFR #######################################################################################\n","CEFR = {}\n","CEFR_train = pd.read_csv('../L2/data/DATA_s/CEFRtrain.csv',index_col=None)\n","CEFR_test = pd.read_csv('../L2/data/DATA_s/CEFRtest.csv',index_col=None)\n","CEFR['train'] = CEFR_train.iloc[:,1:3]\n","CEFR['test'] = CEFR_test.iloc[:,1:3]\n","\n","########## CLEC #######################################################################################\n","CLEC = {}\n","CLEC_train=pd.read_csv('../L2/data/DATA_s/CLECtrain.csv',index_col=None)\n","CLEC_test = pd.read_csv('../L2/data/DATA_s/CLECtest.csv',index_col=None)\n","CLEC['train'] = CLEC_train.iloc[:,1:3]\n","CLEC['test'] = CLEC_test.iloc[:,1:3]\n","\n","########## CLOTH #######################################################################################\n","CLOTH = {}\n","CLOTH_train=pd.read_csv('../L2/data/DATA_s/CLOTHtrain.csv',index_col=None)\n","CLOTH_test = pd.read_csv('../L2/data/DATA_s/CLOTHtest.csv',index_col=None)\n","CLOTH['train'] = CLOTH_train.iloc[:,1:3]\n","CLOTH['test'] = CLOTH_test.iloc[:,1:3]\n","\n","NES = {}\n","NES_train=pd.read_csv('../L2/data/DATA_s/NEStrain.csv',index_col=None)\n","NES_test = pd.read_csv('../L2/data/DATA_s/NEStest.csv',index_col=None)\n","NES['train'] = NES_train.iloc[:,0:2]\n","NES['test'] = NES_test.iloc[:,0:2]\n","\n","########## OSP #######################################################################################\n","OSP = {}\n","OSP_train=pd.read_csv('../L2/data/DATA_s/OSPtrain.csv',index_col=None)\n","OSP_test = pd.read_csv('../L2/data/DATA_s/OSPTest.csv',index_col=None)\n","OSP['train'] = OSP_train.iloc[:,1:3]\n","OSP['test'] = OSP_test.iloc[:,1:3]\n","\n","########## RACE #######################################################################################\n","RACE = {}\n","RACE_train=pd.read_csv('../L2/data/DATA_s/RACEtrain.csv',index_col=None)\n","RACE_test = pd.read_csv('../L2/data/DATA_s/RACEtest.csv',index_col=None)\n","RACE['train'] = RACE_train.iloc[:,1:3]\n","RACE['test'] = RACE_test.iloc[:,1:3]\n","ALL = {'CEFR':CEFR, 'CLEC':CLEC, 'CLOTH':CLOTH, 'NES':NES, 'OSP':OSP,'RACE':RACE}\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4606,"status":"ok","timestamp":1682165943084,"user":{"displayName":"zhiqin li","userId":"14648237152869568839"},"user_tz":-480},"id":"7hfn1OMU_Clh"},"outputs":[],"source":["# feature label\n","########## CEFR #######################################################################################\n","CEFR = {}\n","CEFR_train = pd.read_csv('../L2/data/DATA_s/CEFRtrain.csv',index_col=None)\n","CEFR_test = pd.read_csv('../L2/data/DATA_s/CEFRtest.csv',index_col=None)\n","CEFR_train = CEFR_train.iloc[:,1:3]\n","CEFR_test = CEFR_test.iloc[:,1:3]\n","CEFR_train_feature = pd.read_csv('../L2/data/DATA_feature/splite/[CEFR]with_features_train.csv',index_col=None)\n","CEFR_test_feature = pd.read_csv('../L2/data/DATA_feature/splite/[CEFR]with_features_test.csv',index_col=None)\n","CEFR_feature_train = CEFR_train_feature.iloc[:,1:23]\n","CEFR_feature_test = CEFR_test_feature.iloc[:,1:23]\n","\n","########## CLEC #######################################################################################\n","CLEC = {}\n","CLEC_train=pd.read_csv('../L2/data/DATA_s/CLECtrain.csv',index_col=None)\n","CLEC_test = pd.read_csv('../L2/data/DATA_s/CLECtest.csv',index_col=None)\n","CLEC_train = CLEC_train.iloc[:,1:3]\n","CLEC_test = CLEC_test.iloc[:,1:3]\n","CLEC_train_feature = pd.read_csv('../L2/data/DATA_feature/splite/[CLEC]with_features_train.csv',index_col=None)\n","CLEC_test_feature = pd.read_csv('../L2/data/DATA_feature/splite/[CLEC]with_features_test.csv',index_col=None)\n","CLEC_feature_train = CLEC_train_feature.iloc[:,1:23]\n","CLEC_feature_test = CLEC_test_feature.iloc[:,1:23]\n","\n","########## CLOTH #######################################################################################\n","CLOTH = {}\n","CLOTH_train=pd.read_csv('../L2/data/DATA_s/CLOTHtrain.csv',index_col=None)\n","CLOTH_test = pd.read_csv('../L2/data/DATA_s/CLOTHtest.csv',index_col=None)\n","CLOTH_train = CLOTH_train.iloc[:,1:3]\n","CLOTH_test = CLOTH_test.iloc[:,1:3]\n","CLOTH_train_feature = pd.read_csv('../L2/data/DATA_feature/splite/[CLOTH]with_features_train.csv',index_col=None)\n","CLOTH_test_feature = pd.read_csv('../L2/data/DATA_feature/splite/[CLOTH]with_features_test.csv',index_col=None)\n","CLOTH_feature_train = CLOTH_train_feature.iloc[:,1:23]\n","CLOTH_feature_test = CLOTH_test_feature.iloc[:,1:23]\n","\n","NES = {}\n","NES_train=pd.read_csv('../L2/data/DATA_s/NEStrain.csv',index_col=None)\n","NES_test = pd.read_csv('../L2/data/DATA_s/NEStest.csv',index_col=None)\n","NES_train = NES_train.iloc[:,0:2]\n","NES_test = NES_test.iloc[:,0:2]\n","NES_train_feature = pd.read_csv('../L2/data/DATA_s/[NES2]with_features_train.csv',index_col=None)\n","NES_test_feature = pd.read_csv('../L2/data/DATA_s/[NES2]with_features_test.csv',index_col=None)\n","NES_feature_train = NES_train_feature.iloc[:,2:23]\n","NES_feature_test = NES_test_feature.iloc[:,2:23]\n","\n","########## OSP #######################################################################################\n","OSP = {}\n","OSP_train=pd.read_csv('../L2/data/DATA_s/OSPtrain.csv',index_col=None)\n","OSP_test = pd.read_csv('../L2/data/DATA_s/OSPTest.csv',index_col=None)\n","OSP_train = OSP_train.iloc[:,1:3]\n","OSP_test = OSP_test.iloc[:,1:3]\n","OSP_train_feature = pd.read_csv('../L2/data/DATA_feature/splite/[OSP]with_features_train.csv',index_col=None)\n","OSP_test_feature = pd.read_csv('../L2/data/DATA_feature/splite/[OSP]with_features_test.csv',index_col=None)\n","OSP_feature_train = OSP_train_feature.iloc[:,1:23]\n","OSP_feature_test = OSP_test_feature.iloc[:,1:23]\n","\n","########## RACE #######################################################################################\n","RACE = {}\n","RACE_train=pd.read_csv('../L2/data/DATA_s/RACEtrain.csv',index_col=None)\n","RACE_test = pd.read_csv('../L2/data/DATA_s/RACEtest.csv',index_col=None)\n","RACE_train = RACE_train.iloc[:,1:3]\n","RACE_test = RACE_test.iloc[:,1:3]\n","RACE_train_feature = pd.read_csv('../L2/data/DATA_feature/splite/[RACE]with_features_train.csv',index_col=None)\n","RACE_test_feature = pd.read_csv('../L2/data/DATA_feature/splite/[RACE]with_features_test.csv',index_col=None)\n","RACE_feature_train = RACE_train_feature.iloc[:,1:23]\n","RACE_feature_test = RACE_test_feature.iloc[:,1:23]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#feature label\n","########## CEFR #######################################################################################\n","CEFR = {}\n","CEFR_train = pd.read_csv('../L2/data/DATA_feature/splite/[CEFR]with_features_train.csv',index_col=None)\n","CEFR_test = pd.read_csv('../L2/data/DATA_feature/splite/[CEFR]with_features_test.csv',index_col=None)\n","CEFR['train'] = CEFR_train.iloc[:,1:23]\n","CEFR['test'] = CEFR_test.iloc[:,1:23]\n","\n","########## CLEC #######################################################################################\n","CLEC = {}\n","CLEC_train = pd.read_csv('../L2/data/DATA_feature/splite/[CLEC]with_features_train.csv',index_col=None)\n","CLEC_test = pd.read_csv('../L2/data/DATA_feature/splite/[CLEC]with_features_test.csv',index_col=None)\n","CLEC['train'] = CLEC_train.iloc[:,1:23]\n","CLEC['test'] = CLEC_test.iloc[:,1:23]\n","\n","########## CLOTH #######################################################################################\n","CLOTH = {}\n","CLOTH_train = pd.read_csv('../L2/data/DATA_feature/splite/[CLOTH]with_features_train.csv',index_col=None)\n","CLOTH_test = pd.read_csv('../L2/data/DATA_feature/splite/[CLOTH]with_features_test.csv',index_col=None)\n","CLOTH['train'] = CLOTH_train.iloc[:,1:23]\n","CLOTH['test'] = CLOTH_test.iloc[:,1:23]\n","\n","########## NES #######################################################################################\n","NES = {}\n","NES_train = pd.read_csv('../L2/data/DATA_s/[NES]with_features_train.csv',index_col=None)\n","NES_test = pd.read_csv('../L2/data/DATA_s/[NES]with_features_test.csv',index_col=None)\n","NES['train'] = NES_train.iloc[:,2:23]\n","NES['test'] = NES_test.iloc[:,2:23]\n","\n","########## OSP #######################################################################################\n","OSP = {}\n","OSP_train = pd.read_csv('../L2/data/DATA_feature/splite/[OSP]with_features_train.csv',index_col=None)\n","OSP_test = pd.read_csv('../L2/data/DATA_feature/splite/[OSP]with_features_test.csv',index_col=None)\n","OSP['train'] = OSP_train.iloc[:,1:23]\n","OSP['test'] = OSP_test.iloc[:,1:23]\n","\n","########## RACE #######################################################################################\n","RACE = {}\n","RACE_train = pd.read_csv('../L2/data/DATA_feature/splite/[RACE]with_features_train.csv',index_col=None)\n","RACE_test = pd.read_csv('../L2/data/DATA_feature/splite/[RACE]with_features_test.csv',index_col=None)\n","RACE['train'] = RACE_train.iloc[:,1:23]\n","RACE['test'] = RACE_test.iloc[:,1:23]\n","\n","feature = {'CEFR':CEFR, 'CLEC':CLEC, 'CLOTH':CLOTH, 'NES':NES, 'OSP':OSP,'RACE':RACE}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19715,"status":"ok","timestamp":1682165974458,"user":{"displayName":"zhiqin li","userId":"14648237152869568839"},"user_tz":-480},"id":"TJgErIivq73O","outputId":"3aa60f60-6737-4395-d903-afe8f902d9b6"},"outputs":[],"source":["import os\n","!apt-get install libenchant1c2a\n","path = \"/content/enchant/\"\n","os.makedirs(path, exist_ok=True)\n","os.chdir(path)\n","!tar -zxvf ../L2/data/data/enchant/enchant-1.6.0.tar.gz\n","!/content/enchant-1.6.0/configure --prefix=/usr/local/enchant\n","!make \n","!make install\n","!pip install pyenchant"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1682165978868,"user":{"displayName":"zhiqin li","userId":"14648237152869568839"},"user_tz":-480},"id":"GsJdQ7G_rLd9"},"outputs":[],"source":["import os\n","import re\n","import pandas as pd\n","import numpy as np\n","import string\n","import enchant\n","from tensorflow import keras \n","import pickle\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","dataroot = '/content/DL_Readability/'\n","TOK_PATH=dataroot + 'tokenizer.pickle'\n","LOG_PATH=dataroot + '/logs/fit/'\n","EMBEDDING_PATH = '../L2/data/glove/glove.6B.300d.txt'\n","os.makedirs(dataroot, exist_ok=True)\n","os.makedirs(LOG_PATH, exist_ok=True)\n","MODEL_PATH = '/content/DL_Readability/model/'\n","TRAIN_PATH = '/content/8_Readability/corpus-model-filtered-train.csv'\n","TEST_PATH = '/content/8_Readability/corpus-model-filtered-validate.csv'\n","PREDICT_PATH = '/content/8_Readability//corpus-model-filtered-test.csv'\n","TOK_PATH='/content/DL_Readability/tokenizer.pickle'\n","TOK_PATH='../L2/data/privacy/privacy-policy-historical/tokenizer.pickle'\n","PREDICT_LEVEL=0.5\n","MAX_SEQUENCE_LEN=256\n","MAX_WORDS_LEN=20000\n","EMBED_SIZE=300\n","text_bi_lstm='TEXT_BI_LSTM'\n","text_att_bi_lstm='TEXT_ATT_BI_LSTM'"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vkDF4pqHr3d5"},"source":["Load Data"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682165978868,"user":{"displayName":"zhiqin li","userId":"14648237152869568839"},"user_tz":-480},"id":"5qTZrNWIsBTK"},"outputs":[],"source":["def get_features(text_series, maxlen):\n","    with open(TOK_PATH, 'rb') as handle:\n","        tokenizer = pickle.load(handle)\n","    sequences = tokenizer.texts_to_sequences(text_series)\n","    return pad_sequences(sequences, maxlen=maxlen)\n","\n","def clean(abstracts_tmp,labels_tmp):\n","    print(len(abstracts_tmp) == len(labels_tmp))\n","    reg = re.compile(r'<[^>]+>', re.S)\n","    max_seq_len = 0\n","    all_words = []\n","    dic_en = enchant.Dict(\"en_US\")\n","    t = str.maketrans({key: None for key in string.punctuation})\n","    abstracts = []\n","    labels = []\n","    count = 0\n","    for i in tqdm(range(len(abstracts_tmp)), desc='CLEANING...'):\n","        line = abstracts_tmp[i]\n","        y = labels_tmp[i]\n","        desc = reg.sub('', str(line))\n","        desc = re.sub(r'\\W+', ' ', desc)\n","        desc = desc.translate(t).lower()\n","        desc = desc.split(' ')\n","        x = ''\n","        for word in desc:\n","            if x.find(word) == -1 and word != '' and word != ' ' and re.search('\\d', word) is None and dic_en.check(word):\n","                if word not in all_words:\n","                    all_words.append(word)\n","                x += word+' '\n","        x = x.strip()\n","        if len(x) == 0:\n","            count = count + (i+1)/(i+1)\n","            continue\n","        if len(x.split(' ')) > max_seq_len:\n","            max_seq_len = len(x)\n","        # if y in CLASSES:\n","        abstracts.append(x)\n","        labels.append(y)\n","    print('MAX SEQ LEN:{}'.format(max_seq_len))\n","    print('ALL WORDS:{}'.format(len(all_words)))\n","    return abstracts, labels\n","\n","def load_data(path = '', corpus = None):\n","    if path != '':\n","        newcol=['abstract','Class']\n","        datas = pd.read_csv(path,names=newcol,header = None)\n","        labels_tmp = datas.Class.tolist()\n","        abstracts_tmp = datas.abstract.tolist()\n","    else:\n","      labels_tmp, abstracts_tmp = corpus.Level.tolist(), corpus.Text.tolist()\n","    \n","    abstracts,labels = clean(abstracts_tmp, labels_tmp)\n","    print('DATA LEN:{}'.format(len(abstracts)))\n","    return abstracts, labels"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"r24rC3qIsFgr"},"source":["Get Model"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1682165980493,"user":{"displayName":"zhiqin li","userId":"14648237152869568839"},"user_tz":-480},"id":"tuZPnONb0Dd7"},"outputs":[],"source":["from tensorflow.keras.layers import Embedding, Input, Dropout, Dense, Concatenate\n","from tensorflow.keras.models import Model\n","\n","import sys\n","sys.path.insert(0, \"../L2/data/Keras_Multi_Label_TextClassfication\")\n","\n","from models.text_att_bi_lstm import *\n","from models.text_bi_lstm import *\n","\n","models=[text_att_bi_lstm,text_bi_lstm]\n","for m in models:\n","  globals()[m]\n","\n","def create_embedding_layer(embedding_matrix):\n","    return Embedding(input_dim=MAX_WORDS_LEN,\n","                     output_dim=EMBED_SIZE,\n","                     weights=[embedding_matrix],\n","                     input_length=MAX_SEQUENCE_LEN,\n","                     trainable=False)\n","\n","def get_model(model_type, embedding_matrix, class_num, max_feature_num):\n","    text_input = Input(shape=(MAX_SEQUENCE_LEN,), name='text_input')\n","    feature_input = Input(shape=(max_feature_num,), name='feature_input')\n","    embedding_layer = create_embedding_layer(embedding_matrix)\n","    lstm_output= globals()[model_type](embedding_layer, class_num, MAX_SEQUENCE_LEN).build()(text_input)\n","    merged = Concatenate(axis=1)([lstm_output, lstm_output])\n","    language_feature_layer = Dense(128, activation='relu')(feature_input)\n","    merged = Concatenate(axis=1)([merged, language_feature_layer])\n","    dense = Dense(128, activation='relu')(merged)\n","    dropout = Dropout(0.2)(dense)\n","    output = Dense(class_num, activation='sigmoid')(dropout)\n","    model = Model(inputs=[text_input, feature_input], outputs=output)\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n","    return model\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"EgGCSyEdpLsq"},"source":["Train Model"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682165980494,"user":{"displayName":"zhiqin li","userId":"14648237152869568839"},"user_tz":-480},"id":"HEs27of6tAUB"},"outputs":[],"source":["import sys\n","import datetime\n","import pickle\n","import gc\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.callbacks import (EarlyStopping, ModelCheckpoint,ReduceLROnPlateau)\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tqdm import tqdm\n","\n","def preprocess_labels(labels, encoder=None, categorical=True):\n","    if not encoder:\n","        encoder = LabelEncoder()\n","        encoder.fit(labels)\n","    y = encoder.transform(labels).astype(np.int32)\n","    if categorical:\n","        y = tf.keras.utils.to_categorical(y)\n","    return y, encoder\n","\n","def build_matrix(embeddings_index, word_index):\n","    embedding_matrix = np.zeros((MAX_WORDS_LEN, EMBED_SIZE))\n","    for word, i in tqdm(word_index.items(),desc='BUILD EMBEDDING'):\n","        if i >= MAX_WORDS_LEN:\n","            continue\n","        try:\n","            embedding_vector = embeddings_index[word]\n","        except:\n","            embedding_vector = embeddings_index[\"unknown\"]\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","    return embedding_matrix\n","\n","def get_coefs(word, *arr):\n","    return word, np.asarray(arr, dtype='float32')"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1682165980979,"user":{"displayName":"zhiqin li","userId":"14648237152869568839"},"user_tz":-480},"id":"OCQ_S6Oo91lo"},"outputs":[],"source":["def train(abstract, labels, train_feature, model_type='', Set = ''):\n","    tokenizer = Tokenizer(num_words=MAX_WORDS_LEN, lower=True)\n","    tokenizer.fit_on_texts(abstract)\n","    sequences = tokenizer.texts_to_sequences(abstract)\n","    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LEN)\n","    classes = len(set(labels))\n","    en_labels, _ = preprocess_labels(labels)\n","    uniques, ids = np.unique(labels, return_inverse=True)\n","    with open(TOK_PATH, 'ab') as handle:\n","        pickle.dump(tokenizer, handle, protocol = pickle.HIGHEST_PROTOCOL)\n","    embeddings_index = dict(get_coefs(*o.split(\" \"))\n","                            for o in open(EMBEDDING_PATH))\n","    glove_embedding_matrix = build_matrix(\n","        embeddings_index, tokenizer.word_index)\n","    X_train, X_validation, y_train, y_validation = train_test_split(\n","        data, en_labels, test_size=0.2, random_state=70988860)    \n","    X_feature_train, X_feature_validation, y_feature_train, y_feature_validation = train_test_split(\n","        train_feature, en_labels, test_size=0.2, random_state=70988860)\n","    model = get_model(model_type, glove_embedding_matrix, class_num = classes, max_feature_num=21)\n","\n","    log_dir = LOG_PATH + Set + '/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n","        log_dir=log_dir, histogram_freq=1)\n","\n","    if Set == '':\n","        savepath = MODEL_PATH + Set + \"-\" + str(model_type) + '.h5'\n","    else:\n","        os.makedirs(MODEL_PATH + Set, exist_ok=True)\n","        savepath = MODEL_PATH + Set + '/' + str(model_type) + '.h5'\n","    callbacks = [\n","        ReduceLROnPlateau(monitor='categorical_accuracy'),\n","        ModelCheckpoint(filepath = savepath, save_best_only=True),\n","        tensorboard_callback\n","    ]\n","\n","    history = model.fit((X_train, X_feature_train), \n","                y_train,\n","                epochs=1,\n","                batch_size=64,\n","                verbose=1,\n","                validation_data=([X_validation,X_feature_validation], y_validation),\n","                callbacks=callbacks\n","                )\n","    y_pred = model.predict([X_validation,X_feature_validation])\n","    y_pred_labels = np.argmax(y_pred, axis=1)\n","    y_true_labels = np.argmax(y_validation, axis=1)\n","\n","    acc = accuracy_score(y_true_labels, y_pred_labels)\n","    precision = precision_score(y_true_labels, y_pred_labels, average='macro',zero_division=1)\n","    recall = recall_score(y_true_labels, y_pred_labels, average='macro')\n","    f1 = f1_score(y_true_labels, y_pred_labels, average='macro')\n","    \n","    print(f'Accuracy: {acc:.4f}')\n","    print(f'Precision: {precision:.4f}')\n","    print(f'Recall: {recall:.4f}')\n","    print(f'F1 Score: {f1:.4f}')\n","\n","    val_cat_acc = history.history['val_categorical_accuracy']\n","    best_score = max(val_cat_acc)\n","    best_epoch = val_cat_acc.index(best_score)\n","    print('++**'*10)\n","    print(Set+'-->'+str(model_type).upper()+'-->训练完成')\n","    print('++**'*10)\n","\n","    return str(model_type).upper(), '%.4f' % best_score, best_epoch, uniques\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lpNd201ZtWQT"},"source":["Cross-corpus readability assessment"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":423,"status":"ok","timestamp":1682165990122,"user":{"displayName":"zhiqin li","userId":"14648237152869568839"},"user_tz":-480},"id":"Hu_yAW9AwQpm"},"outputs":[],"source":["def train_and_predict(Train,Test,train_feature,Name=None):\n","    result = []\n","    models = [text_att_bi_lstm,text_bi_lstm]\n","    X_train,y_train = load_data(corpus = Train)\n","    for M in models:\n","      mode_type, best_socre, best_epoch, uniques = train(X_train, y_train, train_feature, model_type = M, Set = Name)\n","      result.append([mode_type, best_socre, best_epoch])\n","    print('| (Classifier) | val_categorical_accuracy | epochs |')\n","    print('| :----------------- | :------------------- | :----- |')\n","    for r in result:\n","        print('| {}                | {}                  | {}       |'.format(r[0],r[1],r[2]))\n","    return uniques"]},{"cell_type":"code","execution_count":137,"metadata":{"executionInfo":{"elapsed":401,"status":"ok","timestamp":1682153765184,"user":{"displayName":"zhiqin li","userId":"14648237152869568839"},"user_tz":-480},"id":"h1pYp0ewnpkj"},"outputs":[],"source":["def TestOther(Path, Test, test_feature, uniques):\n","    if np.shape(test_feature)[0] != np.shape(Test)[0]:\n","      print(\"Sample size mismatch!\")\n","    x, y_ = load_data(corpus=Test)\n","    test_feature = test_feature.values.tolist()\n","    model = tf.keras.models.load_model(Path)\n","    x_text = get_features(x, MAX_SEQUENCE_LEN)\n","    model_inputs =  [x_text] + [np.array(test_feature)]\n","    y_pred_raw = model.predict(model_inputs)\n","    y_pred = np.argmax(y_pred_raw, axis=1)\n","    y_p = [uniques[t] for t in y_pred]\n","    return y_, y_p\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["DL+glove+feature"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":460145,"status":"ok","timestamp":1682153136971,"user":{"displayName":"zhiqin li","userId":"14648237152869568839"},"user_tz":-480},"id":"JS5nbyG-kfAv","outputId":"736ffdad-f467-460e-d535-8d92963784e2"},"outputs":[],"source":["uniques_CEFR = train_and_predict(Train = ALL['CEFR']['train'], Test = None,train_feature=feature['CEFR']['train'], Name = 'CEFR')\n","MODEL_PATH = '/content/DL_Readability/model/CEFR'\n","AllSet = ['CEFR', 'CLEC', 'CLOTH', 'NES', 'OSP', 'RACE']\n","file1 = 'CEFR'\n","os.chdir('../L2/data/Result_DL+glove+feature')\n","datasets = {'CEFR': [CEFR_train,CEFR_test,CEFR_feature_train, CEFR_feature_test],\n","            'CLEC': [CLEC_train,CLEC_test,CLEC_feature_train, CLEC_feature_test],\n","            'CLOTH': [CLOTH_train,CLOTH_test,CLOTH_feature_train, CLOTH_feature_test],\n","            'NES': [NES_train,NES_test,NES_feature_train, NES_feature_test],\n","            'OSP': [OSP_train,OSP_test,OSP_feature_train, OSP_feature_test],\n","            'RACE': [RACE_train,RACE_test,RACE_feature_train, RACE_feature_test]}\n","\n","tmp = {}\n","for f in os.listdir(MODEL_PATH):\n","  TestModelPath = os.path.join(MODEL_PATH,f)\n","  ModelName = f.split('.')[0]\n","  print(TestModelPath)\n","  result = {}\n","  model = tf.keras.models.load_model(TestModelPath)\n","  for data in datasets:\n","    X_train, X_test, feature_train,feature_test= datasets[data]\n","    x, y_ = load_data(corpus=X_test)\n","    test_feature = feature_test.values.tolist()\n","    x_text = get_features(x, MAX_SEQUENCE_LEN)\n","    model_inputs =  [x_text] + [np.array(test_feature)]\n","    y_pred_raw = model.predict(model_inputs)\n","    y_pred = np.argmax(y_pred_raw, axis=1)\n","    y_p = [uniques_CEFR[t] for t in y_pred]\n","    result['Numy'] = y_\n","    result['Numyp'] = y_p\n","    FileName = file1 + '_' + data + '_' + ModelName + '.txt'\n","    df = pd.DataFrame(result)\n","    df.to_csv(FileName,sep=' ',index=0,header=0)\n","print('评估统计结果：',len(os.listdir('../L2/data/Result_DL+glove+feature')))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":465512,"status":"ok","timestamp":1682157650678,"user":{"displayName":"zhiqin li","userId":"14648237152869568839"},"user_tz":-480},"id":"S4jxbwABLhyg","outputId":"6931abd8-f1cf-479c-90a6-1ab973f93d19"},"outputs":[],"source":["uniques_CLEC = train_and_predict(Train = ALL['CLEC']['train'], Test = None,train_feature=feature['CLEC']['train'], Name = 'CLEC')\n","MODEL_PATH = '/content/DL_Readability/model/CLEC'\n","file1 = 'CLEC'\n","os.chdir('../L2/data/Result_DL+glove+feature')\n","tmp = {}\n","for f in os.listdir(MODEL_PATH):\n","  TestModelPath = os.path.join(MODEL_PATH,f)\n","  ModelName = f.split('.')[0]\n","  print(TestModelPath)\n","  result = {}\n","  model = tf.keras.models.load_model(TestModelPath)\n","\n","  for data in datasets:\n","    X_train, X_test, feature_train,feature_test= datasets[data]\n","    x, y_ = load_data(corpus=X_test)\n","    test_feature = feature_test.values.tolist()\n","    x_text = get_features(x, MAX_SEQUENCE_LEN)\n","    model_inputs =  [x_text] + [np.array(test_feature)]\n","    y_pred_raw = model.predict(model_inputs)\n","    y_pred = np.argmax(y_pred_raw, axis=1)\n","    y_p = [uniques_CLEC[t] for t in y_pred]\n","    \n","    result['Numy'] = y_\n","    result['Numyp'] = y_p\n","    FileName = file1 + '_' + data + '_' + ModelName + '.txt'\n","    df = pd.DataFrame(result)\n","    df.to_csv(FileName,sep=' ',index=0,header=0)\n","print('评估统计结果：',len(os.listdir('../L2/data/Result_DL+glove+feature')))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":454628,"status":"ok","timestamp":1682158105289,"user":{"displayName":"zhiqin li","userId":"14648237152869568839"},"user_tz":-480},"id":"3sj5jaewPgZt","outputId":"4cb6635e-8ed2-4c67-bcf5-2478fbccf921"},"outputs":[],"source":["uniques_CLOTH = train_and_predict(Train = ALL['CLOTH']['train'], Test = None,train_feature=feature['CLOTH']['train'], Name = 'CLOTH')\n","MODEL_PATH = '/content/DL_Readability/model/CLOTH'\n","file1 = 'CLOTH'\n","os.chdir('../L2/data/Result_DL+glove+feature')\n","\n","tmp = {}\n","for f in os.listdir(MODEL_PATH):\n","  TestModelPath = os.path.join(MODEL_PATH,f)\n","  ModelName = f.split('.')[0]\n","  print(TestModelPath)\n","  result = {}\n","  model = tf.keras.models.load_model(TestModelPath)\n","  for data in datasets:\n","    X_train, X_test, feature_train,feature_test= datasets[data]\n","    x, y_ = load_data(corpus=X_test)\n","    test_feature = feature_test.values.tolist()\n","    x_text = get_features(x, MAX_SEQUENCE_LEN)\n","    model_inputs =  [x_text] + [np.array(test_feature)]\n","    y_pred_raw = model.predict(model_inputs)\n","    y_pred = np.argmax(y_pred_raw, axis=1)\n","    y_p = [uniques_CLOTH[t] for t in y_pred]\n","    result['Numy'] = y_\n","    result['Numyp'] = y_p\n","    FileName = file1 + '_' + data + '_' + ModelName + '.txt'\n","    df = pd.DataFrame(result)\n","    df.to_csv(FileName,sep=' ',index=0,header=0)\n","print('评估统计结果：',len(os.listdir('../L2/data/Result_DL+glove+feature')))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":449950,"status":"ok","timestamp":1682158555223,"user":{"displayName":"zhiqin li","userId":"14648237152869568839"},"user_tz":-480},"id":"Qjj6zc0_Tj2u","outputId":"bbf17753-10cc-42aa-968a-b33c4eb31eb0"},"outputs":[],"source":["uniques_NES = train_and_predict(Train = ALL['NES']['train'], Test = None,train_feature=feature['NES']['train'], Name = 'NES')\n","MODEL_PATH = '/content/DL_Readability/model/NES'\n","file1 = 'NES'\n","os.chdir('../L2/data/Result_DL+glove+feature')\n","\n","tmp = {}\n","for f in os.listdir(MODEL_PATH):\n","  TestModelPath = os.path.join(MODEL_PATH,f)\n","  ModelName = f.split('.')[0]\n","  print(TestModelPath)\n","  result = {}\n","  model = tf.keras.models.load_model(TestModelPath)\n","\n","  for data in datasets:\n","    X_train, X_test, feature_train,feature_test= datasets[data]\n","    x, y_ = load_data(corpus=X_test)\n","    test_feature = feature_test.values.tolist()\n","    x_text = get_features(x, MAX_SEQUENCE_LEN)\n","    model_inputs =  [x_text] + [np.array(test_feature)]\n","    y_pred_raw = model.predict(model_inputs)\n","    y_pred = np.argmax(y_pred_raw, axis=1)\n","    y_p = [uniques_NES[t] for t in y_pred]\n","    result['Numy'] = y_\n","    result['Numyp'] = y_p\n","    FileName = file1 + '_' + data + '_' + ModelName + '.txt'\n","    df = pd.DataFrame(result)\n","    df.to_csv(FileName,sep=' ',index=0,header=0)\n","print('评估统计结果：',len(os.listdir('../L2/data/Result_DL+glove+feature')))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":456446,"status":"ok","timestamp":1682166635327,"user":{"displayName":"zhiqin li","userId":"14648237152869568839"},"user_tz":-480},"id":"Jxd-pF0KeKR9","outputId":"f329513d-9b56-4b06-c8a6-9c822c2260b1"},"outputs":[],"source":["uniques_OSP = train_and_predict(Train = ALL['OSP']['train'], Test = None,train_feature=feature['OSP']['train'], Name = 'OSP')\n","MODEL_PATH = '/content/DL_Readability/model/OSP'\n","file1 = 'OSP'\n","os.chdir('../L2/data/Result_DL+glove+feature')\n","tmp = {}\n","for f in os.listdir(MODEL_PATH):\n","  TestModelPath = os.path.join(MODEL_PATH,f)\n","  ModelName = f.split('.')[0]\n","  print(TestModelPath)\n","  result = {}\n","  model = tf.keras.models.load_model(TestModelPath)\n","  for data in datasets:\n","    X_train, X_test, feature_train,feature_test= datasets[data]\n","    x, y_ = load_data(corpus=X_test)\n","    test_feature = feature_test.values.tolist()\n","    x_text = get_features(x, MAX_SEQUENCE_LEN)\n","    model_inputs =  [x_text] + [np.array(test_feature)]\n","    y_pred_raw = model.predict(model_inputs)\n","    y_pred = np.argmax(y_pred_raw, axis=1)\n","    y_p = [uniques_OSP[t] for t in y_pred]\n","    result['Numy'] = y_\n","    result['Numyp'] = y_p\n","    FileName = file1 + '_' + data + '_' + ModelName + '.txt'\n","    df = pd.DataFrame(result)\n","    df.to_csv(FileName,sep=' ',index=0,header=0)\n","print('评估统计结果：',len(os.listdir('../L2/data/Result_DL+glove+feature')))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":462507,"status":"ok","timestamp":1682160510383,"user":{"displayName":"zhiqin li","userId":"14648237152869568839"},"user_tz":-480},"id":"wfXRA34H1-xk","outputId":"6dabcc9d-023d-47a2-df4f-b8a2970b1f46"},"outputs":[],"source":["uniques_RACE = train_and_predict(Train = ALL['RACE']['train'], Test = None,train_feature=feature['RACE']['train'], Name = 'RACE')\n","MODEL_PATH = '/content/DL_Readability/model/RACE'\n","file1 = 'RACE'\n","os.chdir('../L2/data/Result_DL+glove+feature')\n","tmp = {}\n","for f in os.listdir(MODEL_PATH):\n","  TestModelPath = os.path.join(MODEL_PATH,f)\n","  ModelName = f.split('.')[0]\n","  print(TestModelPath)\n","  result = {}\n","  model = tf.keras.models.load_model(TestModelPath)\n","\n","  for data in datasets:\n","    X_train, X_test, feature_train,feature_test= datasets[data]\n","    x, y_ = load_data(corpus=X_test)\n","    test_feature = feature_test.values.tolist()\n","    x_text = get_features(x, MAX_SEQUENCE_LEN)\n","    model_inputs =  [x_text] + [np.array(test_feature)]\n","    y_pred_raw = model.predict(model_inputs)\n","    y_pred = np.argmax(y_pred_raw, axis=1)\n","    y_p = [uniques_RACE[t] for t in y_pred]\n","    result['Numy'] = y_\n","    result['Numyp'] = y_p\n","    FileName = file1 + '_' + data + '_' + ModelName + '.txt'\n","    df = pd.DataFrame(result)\n","    df.to_csv(FileName,sep=' ',index=0,header=0)\n","print('评估统计结果：',len(os.listdir('../L2/data/Result_DL+glove+feature')))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["DL+glove"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1XQjlvFIUxPEVqUCilVYJpwtAjbVMH0_Q"},"executionInfo":{"elapsed":388243,"status":"ok","timestamp":1679230693709,"user":{"displayName":"zhiqin li","userId":"14648237152869568839"},"user_tz":-480},"id":"9ogQLYqkUHrY","outputId":"2f46152a-23fa-49f6-e786-c54254f5680f"},"outputs":[],"source":["uniques_CEFR = train_and_predict(Train = ALL['CEFR']['train'], Test = None, Name = 'CEFR')\n","MODEL_PATH = '/content/DL_Readability/model/CEFR'\n","AllSet = {'RACE':RACE, 'CLOTH':CLOTH, 'CLEC':CLEC, 'NES':NES, 'CEFR':CEFR, 'OSP':OSP}\n","file1 = 'CEFR'\n","tmp = {}\n","for f in os.listdir(MODEL_PATH):\n","  TestModelPath = os.path.join(MODEL_PATH,f)\n","  ModelName = f.split('.')[0]\n","  print(TestModelPath)\n","  result = {}\n","  for A in AllSet:\n","    file2 = A\n","    y, y_p = TestOther(Path = TestModelPath, Test = AllSet[A]['test'], uniques = uniques_CEFR)\n","    result['Numy'] = y\n","    result['Numyp'] = y_p\n","    FileName = file1 + '_' + file2 + '_' + ModelName + '.txt'\n","    df = pd.DataFrame(result)\n","    df.to_csv(FileName,sep=' ',index=0,header=0)\n","print('评估统计结果：',len(os.listdir('../L2/data/result')))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["uniques_CLEC = train_and_predict(Train = ALL['CLEC']['train'], Test = None, Name = 'CLEC')\n","MODEL_PATH = '/content/DL_Readability/model/CLEC'\n","AllSet = {'CEFR':CEFR}\n","file1 = 'CLEC'\n","os.chdir('../L2/data/result')\n","tmp = {}\n","for f in os.listdir(MODEL_PATH):\n","  TestModelPath = os.path.join(MODEL_PATH,f)\n","  ModelName = f.split('.')[0]\n","  print(TestModelPath)\n","  result = {}\n","  for A in AllSet:\n","    file2 = A\n","    y, y_p = TestOther(Path = TestModelPath, Test = AllSet[A]['test'], uniques = uniques_CLEC)\n","    result['Numy'] = y\n","    result['Numyp'] = y_p\n","    FileName = file1 + '_' + file2 + '_' + ModelName + '.txt'\n","    df = pd.DataFrame(result)\n","    df.to_csv(FileName,sep=' ',index=0,header=0)\n","print('评估统计结果：',len(os.listdir('../L2/data/result')))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["uniques_CLOTH = train_and_predict(Train = ALL['CLOTH']['train'], Test = None, Name = 'CLOTH')\n","MODEL_PATH = '/content/DL_Readability/model/CLOTH'\n","file1 = 'CLOTH'\n","os.chdir('../L2/data/result')\n","tmp = {}\n","for f in os.listdir(MODEL_PATH):\n","  TestModelPath = os.path.join(MODEL_PATH,f)\n","  ModelName = f.split('.')[0]\n","  print(TestModelPath)\n","  result = {}\n","  for A in AllSet:\n","    count = count + 1\n","    file2 = A\n","    y, y_p = TestOther(Path = TestModelPath, Test = AllSet[A]['test'], uniques = uniques_CLOTH)\n","    result['Numy'] = y\n","    result['Numyp'] = y_p\n","    FileName = file1 + '_' + file2 + '_' + ModelName + '.txt'\n","    df = pd.DataFrame(result)\n","    df.to_csv(FileName,sep=' ',index=0,header=0)\n","print('评估统计结果：',len(os.listdir('../L2/data/result')))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["uniques_NES = train_and_predict(Train = ALL['NES']['train'], Test = None, Name = 'NES')\n","MODEL_PATH = '/content/DL_Readability/model/NES'\n","file1 = 'NES'\n","os.chdir('../L2/data/result')\n","tmp = {}\n","for f in os.listdir(MODEL_PATH):\n","  TestModelPath = os.path.join(MODEL_PATH,f)\n","  ModelName = f.split('.')[0]\n","  print(TestModelPath)\n","  result = {}\n","  for A in AllSet:\n","    file2 = A\n","    y, y_p = TestOther(Path = TestModelPath, Test = AllSet[A]['test'], uniques = uniques_NES)\n","    result['Numy'] = y\n","    result['Numyp'] = y_p\n","    FileName = file1 + '_' + file2 + '_' + ModelName + '.txt'\n","    df = pd.DataFrame(result)\n","    df.to_csv(FileName,sep=' ',index=0,header=0)\n","print('评估统计结果：',len(os.listdir('../L2/data/result')))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["uniques_OSP = train_and_predict(Train = ALL['OSP']['train'], Test = None, Name = 'OSP')\n","MODEL_PATH = '/content/DL_Readability/model/OSP'\n","file1 = 'OSP'\n","os.chdir('../L2/data/result')\n","tmp = {}\n","for f in os.listdir(MODEL_PATH):\n","  TestModelPath = os.path.join(MODEL_PATH,f)\n","  ModelName = f.split('.')[0]\n","  print(TestModelPath)\n","  result = {}\n","  for A in AllSet:\n","    count = count + 1\n","    file2 = A\n","    y, y_p = TestOther(Path = TestModelPath, Test = AllSet[A]['test'], uniques = uniques_OSP)\n","    result['Numy'] = y\n","    result['Numyp'] = y_p\n","    FileName = file1 + '_' + file2 + '_' + ModelName + '.txt'\n","    df = pd.DataFrame(result)\n","    df.to_csv(FileName,sep=' ',index=0,header=0)\n","print('评估统计结果：',len(os.listdir('../L2/data/result')))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59296,"status":"ok","timestamp":1679382660772,"user":{"displayName":"zhiqin li","userId":"14648237152869568839"},"user_tz":-480},"id":"TNpB-4BZcsrv","outputId":"83c02b8b-526d-47b2-b8d7-ef93ad35e712"},"outputs":[],"source":["uniques_RACE = train_and_predict(Train = ALL['RACE']['train'], Test = None,train_feature=feature['RACE']['train'], Name = 'RACE')\n","MODEL_PATH = '/content/DL_Readability/model/RACE'\n","file1 = 'RACE'\n","os.chdir('../L2/data/result')\n","tmp = {}\n","for f in os.listdir(MODEL_PATH):\n","  TestModelPath = os.path.join(MODEL_PATH,f)\n","  ModelName = f.split('.')[0]\n","  print(TestModelPath)\n","  result = {}\n","  for A in AllSet:\n","    count = count + 1\n","    file2 = A\n","    y, y_p = TestOther(Path = TestModelPath, Test = AllSet[A]['test'], uniques = uniques_RACE)\n","    result['Numy'] = y\n","    result['Numyp'] = y_p\n","    FileName = file1 + '_' + file2 + '_' + ModelName + '.txt'\n","    df = pd.DataFrame(result)\n","    df.to_csv(FileName,sep=' ',index=0,header=0)\n","print('评估统计结果：',len(os.listdir('../L2/data/result')))"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
