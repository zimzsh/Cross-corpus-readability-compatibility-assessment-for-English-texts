{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从原始数据集中提取文本和标签"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RACE\n",
    "方法一：从dict数据类型中提取txt和label\n",
    "\n",
    "方法二：该数据集设计了初中和高中试卷中的阅读理解部分数据集，简称RC https://huggingface.co/datasets/race\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file: 13523.txt (list index out of range)\n",
      "Error processing file: 15577.txt (list index out of range)\n",
      "Error processing file: 1582.txt (list index out of range)\n",
      "Error processing file: 18583.txt (list index out of range)\n",
      "Error processing file: 21211.txt (list index out of range)\n",
      "Error processing file: 21566.txt (list index out of range)\n",
      "Error processing file: 23959.txt (list index out of range)\n",
      "Error processing file: 4072.txt (list index out of range)\n",
      "Error processing file: 4228.txt (list index out of range)\n",
      "Error processing file: 6391.txt (list index out of range)\n",
      "Error processing file: 8335.txt (list index out of range)\n",
      "Error processing file: 15561.txt (list index out of range)\n",
      "Error processing file: 17429.txt (list index out of range)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "def tokenize(st):\n",
    "    ans = []\n",
    "    for sent in sent_tokenize(st):\n",
    "        ans += word_tokenize(sent)\n",
    "    return \" \".join(ans).lower()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    difficulty_set = [\"middle\", \"high\"]\n",
    "    # 首先修改 data、raw_data、csv_filename对应路径\n",
    "    data = r\"D:\\00_HanLife\\成果整理\\数据整理\\B_L2_en\\01_RACE\"\n",
    "    raw_data = r\"D:\\00_HanLife\\成果整理\\数据整理\\B_L2_en\\01_RACE\\RACE\"\n",
    "    \n",
    "    csv_filename = r\"D:\\00_HanLife\\成果整理\\数据整理\\B_L2_en\\01_RACE\\race.csv\"  # 指定保存CSV数据的文件名，需要修改到指定文件\n",
    "    \n",
    "    with open(csv_filename, mode='w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"Article\", \"Label\"])  # 写入CSV文件的表头\n",
    "        \n",
    "        for data_set in [\"train\", \"dev\", \"test\"]:\n",
    "            for d in difficulty_set:\n",
    "                new_data_path = os.path.join(data, data_set, d)\n",
    "                new_raw_data_path = os.path.join(raw_data, data_set, d)\n",
    "                \n",
    "                for inf in os.listdir(new_raw_data_path):\n",
    "                    try:\n",
    "                        obj = json.load(open(os.path.join(new_raw_data_path, inf), \"r\"))\n",
    "                        article = tokenize(obj[\"article\"])\n",
    "                        label = d\n",
    "                        \n",
    "                        writer.writerow([article, label])  # 写入文章和标签到CSV文件的一行\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file: {inf} ({e})\")\n",
    "                        continue\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLOTH\n",
    "将dict类型数据中提取txt和label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file: high1209.json (list index out of range)\n",
      "Error processing file: high381.json (list index out of range)\n",
      "Error processing file: high586.json (list index out of range)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "def tokenize(st):\n",
    "    # TODO: The tokenizer's performance is suboptimal\n",
    "    ans = []\n",
    "    for sent in sent_tokenize(st):\n",
    "        ans += word_tokenize(sent)\n",
    "    return \" \".join(ans).lower()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    difficulty_set = [\"middle\", \"high\"]\n",
    "    # 首先修改 data、raw_data、csv_filename对应路径\n",
    "    data = r\"D:\\00_HanLife\\成果整理\\数据整理\\B_L2_en\\02_CLOTH\"\n",
    "    raw_data = r\"D:\\00_HanLife\\成果整理\\数据整理\\B_L2_en\\02_CLOTH\\CLOTH\"\n",
    "    \n",
    "    csv_filename = r\"D:\\00_HanLife\\成果整理\\数据整理\\B_L2_en\\02_CLOTH\\cloth.csv\"  # 指定保存CSV数据的文件名，需要修改到指定文件\n",
    "    \n",
    "    with open(csv_filename, mode='w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"Article\", \"Label\"])  # 写入CSV文件的表头\n",
    "        \n",
    "        for data_set in [\"train\", \"valid\", \"test\"]:\n",
    "            for d in difficulty_set:\n",
    "                new_data_path = os.path.join(data, data_set, d)\n",
    "                new_raw_data_path = os.path.join(raw_data, data_set, d)\n",
    "                \n",
    "                for inf in os.listdir(new_raw_data_path):\n",
    "                    try:\n",
    "                        obj = json.load(open(os.path.join(new_raw_data_path, inf), \"r\"))\n",
    "                        article = tokenize(obj[\"article\"])\n",
    "                        label = d\n",
    "                        \n",
    "                        writer.writerow([article, label])  # 写入文章和标签到CSV文件的一行\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file: {inf} ({e})\")\n",
    "                        continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLEC\n",
    "该数据集包含ST2，ST3，ST4，ST5，ST6五个等级，但是所有文本内容都在一个txt文件中，存在大量的无用信息和标注信息\n",
    "通过观察txt文件中每段文本都是由文章标题和文本内容构成，我们通过文章标题的特殊标识符来分析具体的文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "#from nltk.corpus import stopwords\n",
    "#stopset = set(stopwords.words('english'))\n",
    "\n",
    "def file_max_num(save_path,level):\n",
    "    count = []\n",
    "    for s in os.listdir(os.path.join(save_path,level)):\n",
    "        #print(s)\n",
    "        name = str(os.path.basename(s))\n",
    "        name = name.replace(str(level),'')\n",
    "        name = name.replace('_','')\n",
    "        name = name.replace('.txt','')\n",
    "        count.append(int(name))\n",
    "    return max(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每个语料的情况不同 针对处理\n",
    "#处理CLEC:raw ----- Standard\n",
    "path = r\"C:\\Users\\54554\\Desktop\\clec\"\n",
    "save_path = r\"C:\\Users\\54554\\PycharmProjects\\race_stage1\\data\\posted_data\\all_posted_data2\"\n",
    "for o in os.listdir(path):\n",
    "    open_path = os.path.join(path,o)\n",
    "    if \"2\" in o:\n",
    "        with open(open_path,encoding=\"utf-8\") as f:\n",
    "            data = f.readlines()\n",
    "        for d in data:\n",
    "            if \"<ST\" in d:\n",
    "                data.remove(d)\n",
    "            elif \"\\n\" == d or \" \\n\" == d or \"         \\n\" == d:\n",
    "                data.remove(d)\n",
    "        count = 0\n",
    "        for d in data:\n",
    "            if len(d.split()) >= 70:\n",
    "                count = count + 1\n",
    "                with open(save_path +\"\\\\ST2\\\\\" + str(count) +'.txt',\"w\",encoding=\"utf-8\") as f:\n",
    "                    f.write(d)#将 ST2\n",
    "    elif \"3\" in o:\n",
    "        with open(open_path,encoding=\"utf-8\") as f:\n",
    "            data = f.readlines()\n",
    "        for d in data:\n",
    "            if len(d.split()) < 200:\n",
    "                data.remove(d)\n",
    "            elif \"\\n\" == d:\n",
    "                data.remove(d)\n",
    "        count = 0\n",
    "        for d in data:\n",
    "            if len(d.split()) >= 70:\n",
    "                count = count + 1\n",
    "                with open(save_path +\"\\\\ST3\\\\\" + str(count) +'.txt',\"w\",encoding=\"utf-8\") as f:\n",
    "                    f.write(d)#将ST3\n",
    "    elif \"4\" in o:\n",
    "        with open(open_path,encoding=\"utf-8\") as f:\n",
    "            data = f.readlines()\n",
    "        count = 0\n",
    "        for d in data:\n",
    "            if len(d) >= 140:\n",
    "                count = count + 1\n",
    "                with open(save_path +\"\\\\ST4\\\\\" + str(count) +'.txt',\"w\",encoding=\"utf-8\") as f:\n",
    "                     f.write(d)#ST4\n",
    "    elif \"5\" in o:\n",
    "        with open(open_path,encoding=\"utf-8\") as f:\n",
    "            data = f.readlines()\n",
    "        count = 0\n",
    "        for d in data:\n",
    "            if len(d) >= 140:\n",
    "                count = count + 1\n",
    "                with open(save_path +\"\\\\ST5\\\\\" + str(count) +'.txt',\"w\",encoding=\"utf-8\") as f:\n",
    "                     f.write(d)#ST5\n",
    "    else:\n",
    "        with open(open_path,encoding=\"utf-8\") as f:\n",
    "            data = f.readlines()\n",
    "        count = 0\n",
    "        data_ok = []\n",
    "        for d in data:\n",
    "            if \"<ST\" in d and len(d.split(\" \")) < 50:\n",
    "                #data.remove(d)\n",
    "                continue\n",
    "            elif \"\\n\" == d or \" \\n\" == d or \"         \\n\" == d:\n",
    "                #data.remove(d)\n",
    "                continue\n",
    "            else:data_ok.append(d)\n",
    "        for d in data_ok:\n",
    "            if len(d.split()) > 2:\n",
    "                count = count + 1\n",
    "                with open(save_path +\"\\\\ST6\\\\\" + str(count) +'.txt',\"w\",encoding=\"utf-8\") as f:\n",
    "                    f.write(d)#ST6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def clean_space(text):\n",
    "    \"\"\"\"\n",
    "    处理多余的空格\n",
    "    \"\"\"\n",
    "    match_regex = re.compile(u'[\\u4e00-\\u9fa5。\\.,，:：《》、\\(\\)（）]{1} +(?<![a-zA-Z])|\\d+ +| +\\d+|[a-z A-Z]+')\n",
    "    should_replace_list = match_regex.findall(text)\n",
    "    order_replace_list = sorted(should_replace_list,key=lambda i:len(i),reverse=True)\n",
    "    for i in order_replace_list:\n",
    "        if i == u' ':\n",
    "            continue\n",
    "        new_i = i.strip()\n",
    "        text = text.replace(i,new_i)\n",
    "    return text\n",
    "#得到所有文件\n",
    "data = []\n",
    "for f1 in os.listdir(save_path):\n",
    "    #print(f)\n",
    "    level = f1\n",
    "    path1 = os.path.join(save_path,f1)\n",
    "    for f2 in os.listdir(path1):\n",
    "        #print(f2)\n",
    "        path2 = os.path.join(path1,f2)        \n",
    "        with open(path2,encoding=\"utf-8\") as f:\n",
    "            raw_txt = f.read()\n",
    "        raw_txt = raw_txt.replace('\\n', '')\n",
    "        #------------------------------------------------------\n",
    "        #数据清洗\n",
    "        d = raw_txt\n",
    "        string = \" \".join(s.strip() for s in d.split() if \"<\" not in s and \">\" not in s and \"[\" not in s and ']' not in s)\n",
    "        string = clean_space(string)\n",
    "        #------------------------------------------------------\n",
    "        data.append([string,level])\n",
    "#所有数据保存为clec.csv格式文件\n",
    "name = [\"article\",\"level\"]\n",
    "df_clec = pd.DataFrame(data=data,index=None,columns=name)\n",
    "df_clec.to_csv(\"./data/clec/data_clec.csv\",index = None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSP\n",
    "数据集整理较好，有三个难度等级（Adv、Ele、Int）对应文件夹，每个文件夹下面是具体的文章，同时还提供了三个难度的对比文本。\n",
    "从huggingface中可以直接使用\n",
    "link：https://huggingface.co/datasets/onestop_english\n",
    "code：\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"onestop_english\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NES\n",
    "该数据需要爬虫在对文本进行预处理，参考链接https://github.com/shlomihod/deep-text-eval/blob/master/data/newsela/prepare_newsela.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "\n",
    "HTML_TAG_RE = re.compile('<.*?>', re.DOTALL)\n",
    "MARKDOWN_IMAGE_RE = re.compile('!{0,1}\\[.*?\\]\\s?\\(.*?\\)', re.DOTALL)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "MIN_N_CATEGORY_DATA = 30\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "BASE_URL = 'https://newsela.com/api/v2/articleheader/'\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "\n",
    "class Not200Error(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_newsela_api(part):\n",
    "    r = requests.get(BASE_URL + part, headers=HEADERS)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        raise Not200Error\n",
    "\n",
    "    return r\n",
    "\n",
    "\n",
    "def get_group_df():\n",
    "    page_jsons = []\n",
    "\n",
    "    for page_number in tqdm(itertools.count(start=1)):\n",
    "\n",
    "        result = get_newsela_api('?page={}'.format(page_number)).json()\n",
    "\n",
    "        if not result:\n",
    "            break\n",
    "\n",
    "        page_jsons.append(result)\n",
    "\n",
    "    group_df = pd.concat(map(pd.DataFrame, page_jsons), sort=False)\n",
    "\n",
    "    return group_df\n",
    "\n",
    "\n",
    "def slug2articles(slug):\n",
    "    r = get_newsela_api(slug)\n",
    "\n",
    "    return (pd.DataFrame(\n",
    "        r.json()['articles'])\n",
    "      .assign(slug=slug))\n",
    "\n",
    "\n",
    "def get_text_df(slugs):\n",
    "    articles = []\n",
    "    errors_indices = []\n",
    "\n",
    "    for index, slug in enumerate(tqdm(slugs)):\n",
    "        try:\n",
    "            articles.append(slug2articles(slug))\n",
    "        except Not200Error:\n",
    "            errors_indices.append(index)\n",
    "\n",
    "\n",
    "    text_df = pd.concat(articles)\n",
    "\n",
    "    return text_df, errors_indices\n",
    "\n",
    "def remove_too_small_categories(text_df):\n",
    "    category_counts = text_df['y'].value_counts()\n",
    "    categories_to_keep = category_counts.index[category_counts >= MIN_N_CATEGORY_DATA]\n",
    "\n",
    "    print('Removed Data:')\n",
    "    pprint(text_df[\n",
    "            ~text_df['y'].isin(categories_to_keep)\n",
    "        ]['y'].value_counts())\n",
    "\n",
    "    text_df = text_df[text_df['y'].isin(categories_to_keep)]\n",
    "    return text_df\n",
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    return HTML_TAG_RE.sub(' ', text)\n",
    "\n",
    "\n",
    "def remove_markdown_image_tags(text):\n",
    "    return MARKDOWN_IMAGE_RE.sub(' ', text)\n",
    "\n",
    "\n",
    "def print_data_stats(text_df, train_df, test_df):\n",
    "    dfs = [text_df, train_df, test_df]\n",
    "\n",
    "    text_counts = text_df['y'].value_counts()\n",
    "    train_counts = train_df['y'].value_counts()\n",
    "    test_counts = test_df['y'].value_counts()\n",
    "\n",
    "    print('Categorial y')\n",
    "    print(pd.DataFrame({\n",
    "        '#text': text_counts,\n",
    "        '%text': (100 * text_counts / text_counts.sum()).round(2),\n",
    "        '#train': train_counts,\n",
    "        '%train': (100 * train_counts / train_counts.sum()).round(2),\n",
    "        '#test': test_counts,\n",
    "        '%test': (100 * test_counts / test_counts.sum()).round(2),\n",
    "    }).sort_index())\n",
    "\n",
    "    print()\n",
    "\n",
    "    stats_d = {'name': ['text', 'train', 'test'],\n",
    "             '#': [len(df['y_lexile']) for df in dfs],\n",
    "             'min': [df['y_lexile'].min() for df in dfs],\n",
    "             'max': [df['y_lexile'].max() for df in dfs],\n",
    "             'mean': [df['y_lexile'].mean() for df in dfs],\n",
    "             'std': [df['y_lexile'].std() for df in dfs]\n",
    "            }\n",
    "\n",
    "    stats_df = pd.DataFrame(stats_d)\n",
    "    stats_df.set_index('name')\n",
    "\n",
    "    print('Continuous y_lexile')\n",
    "    pprint(stats_df)\n",
    "    print()\n",
    "    print('train-test Kolmogorov-Smirnov p-value', stats.ks_2samp(train_df['y_lexile'],\n",
    "                                                               test_df['y_lexile']).pvalue)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_corpus():\n",
    "\n",
    "    print('Getting slugs...')\n",
    "    group_df = get_group_df()\n",
    "\n",
    "    print('Filterig non english slugs...')\n",
    "    print(group_df['language'].value_counts())\n",
    "    group_df = group_df[group_df['language'] == 'en']\n",
    "    print(group_df['language'].value_counts())\n",
    "\n",
    "    # it makes a problem to the parser\n",
    "    # https://github.com/nikitakit/self-attentive-parser/issues/5\n",
    "    print('Removing US-Constitution Text...')\n",
    "    group_df = group_df[group_df['slug'] != 'US-Constitution']\n",
    "    print('#Groups =', len(group_df))\n",
    "\n",
    "\n",
    "    print('Getting Texts...')\n",
    "    text_df, errors_indices = get_text_df(group_df['slug'])\n",
    "\n",
    "    if errors_indices:\n",
    "        print('Error Indices:', errors_indices)\n",
    "\n",
    "        print('Retrying Getting Texts...')\n",
    "        retry_text_df, retry_errors_indices = get_text_df(group_df.loc[errors_indices, 'slug'])\n",
    "\n",
    "        print('Error Indices:', retry_errors_indices)\n",
    "\n",
    "        print('Concating Texts...')\n",
    "        text_df = pd.concat([text_df, retry_text_df])\n",
    "\n",
    "    text_df = text_df.rename({'lexile_level': 'y_lexile', 'grade_level': 'y'}, axis=1)\n",
    "    text_df['y'] = text_df['y'].astype(int)\n",
    "    print('#Texts =', len(text_df))\n",
    "\n",
    "    print('Removing Too Small Categories...')\n",
    "    text_df = remove_too_small_categories(text_df)\n",
    "    print('#Texts =', len(text_df))\n",
    "\n",
    "    print('Removing HTML Tags, Markdown Image Tags, and Extra Dashes...')\n",
    "    text_df['text'] = (text_df['text']\n",
    "         .apply(remove_html_tags)\n",
    "         .apply(remove_markdown_image_tags).str.replace('----', ' '))\n",
    "\n",
    "    print('Reset Index...')\n",
    "    text_df = text_df.reset_index(drop=True)\n",
    "\n",
    "    print('Train-Test Split...')\n",
    "    train_df, test_df = train_test_split(text_df,\n",
    "                                     test_size=TEST_SIZE,\n",
    "                                     shuffle=True,\n",
    "                                     stratify=text_df['y'],\n",
    "                                     random_state=RANDOM_STATE)\n",
    "\n",
    "    print_data_stats(text_df, train_df, test_df)\n",
    "    return text_df, train_df, test_df, text_df, group_df\n",
    "\n",
    "\n",
    "print('Getting slugs...')\n",
    "group_df = get_group_df()\n",
    "print('Filterig non english slugs...')\n",
    "print(group_df['language'].value_counts())\n",
    "group_df = group_df[group_df['language'] == 'en']\n",
    "print(group_df['language'].value_counts())\n",
    "print('Removing US-Constitution Text...')\n",
    "group_df = group_df[group_df['slug'] != 'US-Constitution']\n",
    "print('#Groups =', len(group_df))\n",
    "\n",
    "\n",
    "print('Getting Texts...')\n",
    "text_df, errors_indices = get_text_df(group_df['slug'])\n",
    "#最终得到text_df文件，用于分析"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CEFR\n",
    "这个有五个等级难度A1-A2-B1-B2-C1-C2,原始数据集缺少\n",
    "已处理数据：https://github.com/akkikiki/diff_joint_estimate/tree/master/data/processed\n",
    "加载数据：https://github.com/akkikiki/diff_joint_estimate/blob/master/src/data/datasets.py\n",
    "\n",
    "总结第一步得到数据集：\n",
    "!wget https://s3-eu-west-1.amazonaws.com/ilexir-website-media/Readability_dataset.tar.gz\n",
    "!tar xvf Readability_dataset.tar.gz -C data/raw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Tuple, List\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import List, Optional\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "class DatasetSplit(Enum):\n",
    "    TRAIN = 'train'\n",
    "    DEV = 'dev'\n",
    "    TEST = 'test'\n",
    "    UNLABELED = 'unlabeled'\n",
    "\n",
    "class Word:\n",
    "    word_str: str\n",
    "    label: Optional[str]\n",
    "    docs: List[str]\n",
    "    split: DatasetSplit\n",
    "    word_id: Optional[int] = None\n",
    "    doc_ids: Optional[List[int]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    doc_str: str\n",
    "    label: Optional[str]\n",
    "    text: str\n",
    "    words: List[str]\n",
    "    split: DatasetSplit\n",
    "    doc_id: Optional[int] = None\n",
    "    word_ids: Optional[List[int]] = None\n",
    "\n",
    "UK2US_WORDS = {}\n",
    "def normalize_tokens(tokens: List[str]) -> List[str]:\n",
    "    tokens_normalized = []\n",
    "    for token in tokens:\n",
    "        if not re.match(r'[A-Za-z]+', token):\n",
    "            continue\n",
    "        token = token.lower()\n",
    "        token = UK2US_WORDS.get(token, token)\n",
    "\n",
    "        tokens_normalized.append(token)\n",
    "\n",
    "    return tokens_normalized\n",
    "\n",
    "import hashlib\n",
    "\n",
    "EXAM2CEFR = {\n",
    "    \"CAE\": \"C1\",\n",
    "    \"CPE\": \"C2\",\n",
    "    \"PET\": \"B1\",\n",
    "    \"FCE\": \"B2\",\n",
    "    \"KET\": \"A2\"\n",
    "}\n",
    "\n",
    "\n",
    "def get_dataset_split(string_id: str, training_portion: int = 10) -> DatasetSplit:\n",
    "    \"\"\"Given some string ID, return the dataset split (train/dev/test)\"\"\"\n",
    "    if not 1 <= training_portion <= 10:\n",
    "        raise ValueError(f'Invalid training portion: {training_portion}')\n",
    "\n",
    "    md5_hex = hashlib.md5(string_id.encode('utf-8')).hexdigest()\n",
    "    hashed = int(md5_hex, 16) % 20\n",
    "    if hashed < 3:\n",
    "        return DatasetSplit.TEST\n",
    "    elif hashed < 6:\n",
    "        return DatasetSplit.DEV\n",
    "    else:\n",
    "        if training_portion == 10:\n",
    "            return DatasetSplit.TRAIN\n",
    "\n",
    "        training_md5 = hashlib.md5(('salt'+string_id).encode('utf-8')).hexdigest()\n",
    "        training_hashed = int(training_md5, 16) % 10\n",
    "        if training_hashed < training_portion:\n",
    "            return DatasetSplit.TRAIN\n",
    "        else:\n",
    "            return DatasetSplit.UNLABELED\n",
    "\n",
    "\n",
    "def read_cefrj_wordlist(training_portion: int = 10) -> Iterable[Word]:\n",
    "    # TODO: deal with polysemy\n",
    "    with open('data/processed/cefrj_wordlist/all.csv', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            cefr = row['CEFR']\n",
    "            word_str = row['headword']\n",
    "            # if there are multiple spellings, pick the first one\n",
    "            if '/' in word_str:\n",
    "                word_str = word_str.split('/')[0]\n",
    "\n",
    "            word = Word(\n",
    "                word_str=word_str,\n",
    "                label=cefr,\n",
    "                docs=[],\n",
    "                split=get_dataset_split(f'cefrj:{word_str}',\n",
    "                                        training_portion=training_portion)\n",
    "            )\n",
    "            yield word\n",
    "\n",
    "\n",
    "def split_document_to_paragraphs(filename: str) \\\n",
    "        -> Iterable[Tuple[int, str, List[str]]]:\n",
    "    \"\"\"Read a document from filename, and yield tuples\n",
    "        (paragraph_no, paragraph, tokens), where\n",
    "        paragraph_no (int): the 0-base paragraph number,\n",
    "        paragraph (str): the paragraph string, and\n",
    "        tokens (list of strs): tokenized sentence\"\"\"\n",
    "    with open(filename) as f:\n",
    "        lines = f.read()\n",
    "    paragraphs = lines.split(\"\\n\\n\")\n",
    "\n",
    "    for para_id, paragraph in enumerate(paragraphs):\n",
    "        if len(paragraph) < 64:\n",
    "            continue\n",
    "        tokens = normalize_tokens(word_tokenize(paragraph))\n",
    "        yield para_id, paragraph, tokens\n",
    "\n",
    "\n",
    "def read_cambridge_readability_dataset(training_portion: int = 10) -> Iterable[Document]:\n",
    "    ROOT_DIR = \"data/raw/Readability_dataset/\"\n",
    "    HEADERS = set([\"A.\", \"B.\", \"C.\"])\n",
    "\n",
    "    for filename in Path(ROOT_DIR).glob('**/*.txt'):\n",
    "        exam_name = os.path.basename(os.path.dirname(filename))\n",
    "        file_id = os.path.basename(filename).split('.')[0]\n",
    "        cefr = EXAM2CEFR[exam_name]\n",
    "\n",
    "        for para_id, paragraph, tokens in split_document_to_paragraphs(filename):\n",
    "            if paragraph in HEADERS:\n",
    "                continue\n",
    "\n",
    "            doc_str = f'{exam_name}-{file_id}-{para_id}'\n",
    "\n",
    "            doc = Document(\n",
    "                doc_str=doc_str,\n",
    "                label=cefr,\n",
    "                text=paragraph,\n",
    "                words=tokens,\n",
    "                split=get_dataset_split(f'crd:{doc_str}',\n",
    "                                        training_portion=training_portion)\n",
    "            )\n",
    "            yield doc\n",
    "\n",
    "\n",
    "def read_a1_passages(training_portion: int = 10) -> Iterable[Document]:\n",
    "    ROOT_DIR = 'data/processed/a1_passages/'\n",
    "\n",
    "    for filename in Path(ROOT_DIR).glob('*.txt'):\n",
    "        file_id = os.path.basename(filename).split('.')[0]\n",
    "\n",
    "        for paragraph_no, paragraph, tokens in split_document_to_paragraphs(filename):\n",
    "            doc_str = f'A1-{file_id}-{paragraph_no}'\n",
    "            doc = Document(\n",
    "                doc_str=doc_str,\n",
    "                label='A1',\n",
    "                text=paragraph,\n",
    "                words=tokens,\n",
    "                split=get_dataset_split(f'a1:{doc_str}',\n",
    "                                        training_portion=training_portion)\n",
    "            )\n",
    "            yield doc\n",
    "\n",
    "\n",
    "def read_efcamdat_dataset(efcamdat_file_path: str, sent_threshold: int = 2) -> Iterable[Document]:\n",
    "    with open(efcamdat_file_path) as f:\n",
    "        for line_no, line in enumerate(f):\n",
    "            if line_no % 10000 == 0:\n",
    "                print(f'Processed {line_no} lines...', file=sys.stderr)\n",
    "\n",
    "            data = json.loads(line)\n",
    "            essay_id = int(data['id'])\n",
    "\n",
    "            text = data.get('corrected_text')\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            for sent_no, sentence in enumerate(sent_tokenize(text)):\n",
    "                if sent_no > sent_threshold:\n",
    "                    break\n",
    "                tokens = normalize_tokens(word_tokenize(sentence))\n",
    "                doc = Document(\n",
    "                    doc_str=f'efcamdat{essay_id}-{sent_no}',\n",
    "                    label=None,     # unlabeled instance\n",
    "                    text=sentence,\n",
    "                    words=tokens,\n",
    "                    split=DatasetSplit.UNLABELED\n",
    "                )\n",
    "                yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--training-portion', type=int, default=10,\n",
    "                    help='Specify the amount of training data between 1 (10%) and 10 (100%).')\n",
    "args = parser.parse_args()\n",
    "docs = list(itertools.chain(\n",
    "        read_cambridge_readability_dataset(training_portion=args.training_portion),\n",
    "        read_a1_passages(training_portion=args.training_portion)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eduRB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
