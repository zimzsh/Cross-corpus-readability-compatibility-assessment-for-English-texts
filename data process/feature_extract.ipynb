{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools as it\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Avg_words_per_sentence \n",
    "- Avg_syllables_per_word \n",
    "- Complex_word_percent   \n",
    "- Difficult_word_percent \n",
    "- Long_sent_percent      \n",
    "- Long_word_percent      \n",
    "- Avg_letters_per_word   \n",
    "- Comma_percent          \n",
    "- Proper_noun_percent    \n",
    "- Noun_percent           \n",
    "- Pronoun_percent        \n",
    "- Conj_percent           \n",
    "\n",
    "- Tokens            \n",
    "- Words             \n",
    "- Sentences         \n",
    "- N_words           \n",
    "- N_sentences       \n",
    "- N_syllables       \n",
    "- N_polysyllables   \n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pyphen\n",
    "import benepar\n",
    "\n",
    "SPACY_MODEL = \"en_core_web_sm\"\n",
    "import en_core_web_sm\n",
    "\n",
    "def _get_words(x):\n",
    "    words = [token.text for token in x if token.is_punct != True]\n",
    "    return words\n",
    "\n",
    "def words_and_sentences(df):\n",
    "    nlp = spacy.load('en_core_web_sm', exclude=['parser', 'ner'])\n",
    "    nlp.add_pipe('sentencizer')    \n",
    "    df['Tokens'] = df['Text'].apply(lambda x: nlp(x))    \n",
    "    df['Words'] = df['Tokens'].apply(_get_words)    \n",
    "    df['Sentences'] = df['Tokens'].apply(lambda x: list(x.sents))    \n",
    "    df['N_words'] = df['Words'].apply(lambda x: len(x))    \n",
    "    df['N_sentences'] = df['Sentences'].apply(lambda x: len(x))    \n",
    "    df[\"Avg_words_per_sentence\"] = df[\"N_words\"] / df[\"N_sentences\"]    \n",
    "    return df\n",
    "\n",
    "def _count_hyphens(text, dic):\n",
    "    return dic.inserted(text).count(\"-\")\n",
    "\n",
    "def syllables(df):\n",
    "    dic = pyphen.Pyphen(lang='en_EN')\n",
    "    df[\"N_hyphens\"] = df[\"Text\"].apply(lambda x: _count_hyphens(x, dic))\n",
    "    df[\"N_syllables\"] = df[\"N_words\"] + df[\"N_hyphens\"]\n",
    "    df[\"Avg_syllables_per_word\"] = df[\"N_syllables\"] / df[\"N_words\"]\n",
    "    df.drop(columns=[\"N_hyphens\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "def _get_dale_chall_easy_words():\n",
    "    easy_words = set()\n",
    "    with open(\"dale_chall_easy_word_list.txt\") as file:\n",
    "        lines = [line.rstrip('\\n') for line in file]\n",
    "        for line in lines:\n",
    "            easy_words.add(line.lower())\n",
    "    return easy_words\n",
    "\n",
    "\n",
    "def _get_num_difficult_words(text, easy_words):\n",
    "    n = 0\n",
    "    for word in text:\n",
    "        if word.lower() not in easy_words:\n",
    "            n += 1\n",
    "    return n\n",
    "\n",
    "\n",
    "def difficult_words_pct(df):    \n",
    "    easy_words = _get_dale_chall_easy_words()    \n",
    "    df[\"Difficult_word_percent\"] = df[\"Words\"].apply(lambda x: _get_num_difficult_words(x, easy_words)) / df[\"N_words\"]    \n",
    "    return df\n",
    "\n",
    "\n",
    "# POLYSYLLABLES (WORDS WITH 3 OR MORE SYLLABLES)\n",
    "\n",
    "def _count_polysyllables(words, dic):\n",
    "    n_complex = 0    \n",
    "    for word in words:\n",
    "        # if the word has more than 3 or more syllables it will have 2 or more hyphens\n",
    "        if dic.inserted(word).count(\"-\") >= 2:\n",
    "            n_complex += 1    \n",
    "    return n_complex\n",
    "\n",
    "\n",
    "def polysyllables(df):   \n",
    "    dic = pyphen.Pyphen(lang='en_EN')\n",
    "    # use pyphen to find the number of polysyllables\n",
    "    df[\"N_polysyllables\"] = df[\"Words\"].apply(lambda x: _count_polysyllables(x, dic))    \n",
    "    return df\n",
    "\n",
    "# PERCENTAGE OF COMPLEX WORDS (GUNNING FOG)\n",
    "\n",
    "def complex_words_pct(df):   \n",
    "    df[\"Complex_word_percent\"] = df[\"N_polysyllables\"] / df[\"N_words\"]    \n",
    "    return df\n",
    "\n",
    "\n",
    "# PERCENTAGE OF LONG SENTENCES (LONGER THAN 25 WORDS)\n",
    "\n",
    "def _get_n_long_sent(sentences):\n",
    "    n = 0\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > 25:\n",
    "            n += 1\n",
    "    return n\n",
    "\n",
    "\n",
    "def long_sent_pct(df):   \n",
    "    df[\"Long_sent_percent\"] = df[\"Sentences\"].apply(_get_n_long_sent) / df[\"N_sentences\"]    \n",
    "    return df\n",
    "\n",
    "\n",
    "# PERCENTAGE OF LONG WORDS (LONGER THAN 8 CHARACTERS)\n",
    "def _get_n_long_word(words):\n",
    "    n = 0\n",
    "    for word in words:\n",
    "        if len(word) > 8:\n",
    "            n += 1\n",
    "    return n\n",
    "\n",
    "\n",
    "def long_word_pct(df):   \n",
    "    # get percentage\n",
    "    df[\"Long_word_percent\"] = df[\"Words\"].apply(_get_n_long_word) / df[\"N_words\"]    \n",
    "    return df\n",
    "\n",
    "\n",
    "def _get_n_letters(words):\n",
    "    n = 0\n",
    "    for word in words:\n",
    "        n += len(word)\n",
    "    return n\n",
    "\n",
    "\n",
    "def avg_letters_per_word(df):   \n",
    "    df[\"Avg_letters_per_word\"] = df[\"Words\"].apply(_get_n_letters) / df[\"N_words\"]    \n",
    "    return df\n",
    "\n",
    "\n",
    "def _get_n_comma_sent(sentences):\n",
    "    n = 0\n",
    "    for sentence in sentences:\n",
    "        if str(sentence).find(\",\") != -1:\n",
    "            n += 1\n",
    "    return n\n",
    "\n",
    "\n",
    "def comma_pct(df):   \n",
    "    # get percentage\n",
    "    df[\"Comma_percent\"] = df[\"Sentences\"].apply(_get_n_comma_sent) / df[\"N_sentences\"]    \n",
    "    return df\n",
    "\n",
    "\n",
    "def _get_n_pos(tokens, pos_list):\n",
    "    n = 0\n",
    "    for token in tokens:\n",
    "        for pos in pos_list:\n",
    "            if token.pos_ == pos:\n",
    "                n += 1\n",
    "    return n\n",
    "\n",
    "\n",
    "def pos_features(df):    \n",
    "    pos_list = [\"NOUN\", \"PROPN\"]\n",
    "    df[\"Noun_percent\"] = df[\"Tokens\"].apply(lambda x: _get_n_pos(x, pos_list)) / df[\"N_words\"]    \n",
    "    pos_list = [\"PROPN\"]\n",
    "    df[\"Proper_noun_percent\"] = df[\"Tokens\"].apply(lambda x: _get_n_pos(x, pos_list))/ df[\"N_words\"]    \n",
    "    pos_list = [\"PRON\"]\n",
    "    df[\"Pronoun_percent\"] = df[\"Tokens\"].apply(lambda x: _get_n_pos(x, pos_list)) / df[\"N_words\"]    \n",
    "    pos_list = [\"CONJ\", \"CCONJ\"]\n",
    "    df[\"Conj_percent\"] = df[\"Tokens\"].apply(lambda x: _get_n_pos(x, pos_list)) / df[\"N_words\"]    \n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_aux_features(df):\n",
    "   \n",
    "    df.drop(columns=[\"Tokens\", \"Words\", \"Sentences\", \"N_words\", \"N_sentences\", \"N_syllables\", \"N_polysyllables\"], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     e:\\Anaconda\\nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- NP_per_sent\n",
    "- VP_per_sent\n",
    "- PP_per_sent\n",
    "- SBAR_per_sent\n",
    "- SBARQ_per_sent\n",
    "- avg_NP_size\n",
    "- avg_VP_size\n",
    "- avg_PP_size\n",
    "- avg_parse_tree\n",
    "\n",
    "\"\"\"\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import benepar\n",
    "from benepar import BeneparComponent, NonConstituentException\n",
    "benepar.download('benepar_en3')\n",
    "\n",
    "def _parse_tree_height(sent):\n",
    "    \n",
    "    children = list(sent._.children)\n",
    "    if not children:\n",
    "        return 0\n",
    "    else:\n",
    "        return max(_parse_tree_height(child) for child in children) + 1\n",
    "\n",
    "\n",
    "def _get_constituents(tokens):\n",
    "    const_counter = Counter()\n",
    "    const_lengths = defaultdict(list)\n",
    "\n",
    "    for sentence in tokens.sents:\n",
    "        for const in sentence._.constituents:\n",
    "            # add constituent to constituent counter\n",
    "            const_counter.update(Counter(const._.labels))\n",
    "            \n",
    "            # append the length of the constituent\n",
    "            for label in const._.labels:\n",
    "                const_lengths[label].append(len(const))\n",
    "    \n",
    "    # for each constituent, get average of constituent's lengths\n",
    "    const_avgs = defaultdict(int)\n",
    "    for key in const_lengths.keys():\n",
    "        avg = 0.0\n",
    "        for length in const_lengths[key]: \n",
    "            avg += length\n",
    "        avg /= len(const_lengths[key])\n",
    "        \n",
    "        const_avgs[key] = avg\n",
    "         \n",
    "    return const_counter, const_avgs\n",
    "\n",
    "\n",
    "def _get_parse_tree_height(tokens):\n",
    "   \n",
    "    avg_parse_tree_height = 0.0\n",
    "    \n",
    "    for sentence in tokens.sents:\n",
    "        avg_parse_tree_height += _parse_tree_height(sentence)\n",
    "        \n",
    "    n_sentences = len(list(tokens.sents))\n",
    "    avg_parse_tree_height /= n_sentences\n",
    "    \n",
    "    return avg_parse_tree_height, n_sentences\n",
    "\n",
    "\n",
    "def _get_parse_tree_features(tokens):\n",
    "    const_counter, const_avgs = _get_constituents(tokens)\n",
    "    avg_parse_tree_height, n_sentences = _get_parse_tree_height(tokens)\n",
    "    \n",
    "    NP_per_sent = const_counter['NP'] / n_sentences\n",
    "    VP_per_sent = const_counter['VP'] / n_sentences\n",
    "    PP_per_sent = const_counter['PP'] / n_sentences\n",
    "    SBAR_per_sent = const_counter['SBAR'] / n_sentences\n",
    "    SBARQ_per_sent = const_counter['SBARQ'] / n_sentences\n",
    "    avg_NP_size = const_avgs['NP']\n",
    "    avg_VP_size = const_avgs['VP']\n",
    "    avg_PP_size = const_avgs['PP']\n",
    "    avg_parse_tree = avg_parse_tree_height\n",
    "    \n",
    "    return NP_per_sent, VP_per_sent, PP_per_sent, \\\n",
    "        SBAR_per_sent, SBARQ_per_sent, avg_NP_size, \\\n",
    "        avg_VP_size, avg_PP_size, avg_parse_tree\n",
    "    \n",
    "\n",
    "def parse_tree_features(df):\n",
    "    nlp = en_core_web_sm.load(disable=['ner'])\n",
    "    if spacy.__version__.startswith('2'):\n",
    "        nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
    "    else:\n",
    "        nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
    "    # parse text\n",
    "    df['B_Tokens'] = df['Text'].apply(lambda x: nlp(x))\n",
    "    \n",
    "    # get features\n",
    "    df['NP_per_sent'], df['VP_per_sent'], df['PP_per_sent'], \\\n",
    "    df['SBAR_per_sent'], df['SBARQ_per_sent'], df['avg_NP_size'], \\\n",
    "    df['avg_VP_size'], df['avg_PP_size'], df['avg_parse_tree'] = zip(*df['B_Tokens'].map(_get_parse_tree_features))\n",
    "    \n",
    "    # remove B_Tokens\n",
    "    df.drop(columns=[\"B_Tokens\"], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\lib\\site-packages\\torch\\distributions\\distribution.py:44: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    }
   ],
   "source": [
    "datasets=['CEFR','CLEC','CLOTH','NES','OSP','RACE']\n",
    "\n",
    "for data in datasets:\n",
    "    df = pd.read_csv(\"../L2/data/\"+str(data)+\".csv\", index_col = 0)\n",
    "    df['Text'] = df['Text'].astype(str)\n",
    "    df = words_and_sentences(df)\n",
    "    df = syllables(df)dd\n",
    "    df = difficult_words_pct(df)\n",
    "    df = polysyllables(df)\n",
    "    df = complex_words_pct(df)\n",
    "    df = long_sent_pct(df)\n",
    "    df = long_word_pct(df)\n",
    "    df = avg_letters_per_word(df)\n",
    "    df = comma_pct(df)\n",
    "    df = pos_features(df)\n",
    "    df = remove_aux_features(df)\n",
    "    df = parse_tree_features(df)\n",
    "    df.to_csv(\"../L2/feature/\"+str(data)+\"_with_features.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
